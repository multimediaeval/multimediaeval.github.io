---
layout: edition
title: MediaEval 2023
year: 2023
permalink: /editions/2023/
---

The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. Participants address these challenges by creating algorithms for retrieval, analysis, and exploration. Solutions are systematically compared using a common evaluation procedure, making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society. 

MediaEval goes beyond other benchmarks and data science challenges in that it also pursues a “Quest for Insight” (Q4I). With Q4I we push beyond only striving to improve evaluation scores to also working to achieve deeper understanding about the challenges. For example, properties of the data,  strengths and weaknesses of particular types of approaches, and observations about the evaluation procedure.

The MediaEval 2023 Workshop will be held 1-2 February, collocated with [MMM 2024](https://mmm2024.org) in Amsterdam, Netherlands and also online. 

### Preliminary Task schedule 
* July-September 2023: Validation data release
* October 2023: Test data release
* End November 2023: Runs due
* Mid December 2023: Working notes papers due
* 1-2 February 2024: MediaEval 2023 Workshop, Collocated with [MMM 2024](https://mmm2024.org) in Amsterdam, Netherlands and Online.

##### The MediaEval Coordination Committee (2023)
* Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania
* Steven Hicks, SimulaMet, Norway
* Martha Larson, Radboud University, Netherlands (Overall coordinator and main contact person)

MediaEval is grateful for the support of [ACM Special Interest Group on Multimedia](http://sigmm.org/)

<img src="https://multimediaeval.github.io/editions/2020/docs/sigmmlogo.gif" width=150/>
