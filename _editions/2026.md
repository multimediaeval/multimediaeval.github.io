---
layout: edition
title: MediaEval 2026
year: 2026
permalink: /editions/2026/
---

The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. 
Participants address these challenges by creating algorithms for analyzing, exploring and accessing information in the data. Solutions are systematically compared using a common evaluation procedure, 
making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society.

MediaEval goes beyond other benchmarks and data science challenges in that it also pursues a “Quest for Insight” (Q4I). With Q4I we push beyond only striving to improve evaluation 
scores to also working to achieve deeper understanding about the challenges. For example, characteristics of the data, strengths and weaknesses of particular types of approaches, and observations 
about the evaluation procedure.

##### Registration:
Signup for MediaEval 2026 opens in January.

##### MediaEval 2026 Schedule:
* Registration for task participation opens: January 2026
* Test data release: 1 March 2026
* Runs due: 1 May 2026
* Working notes papers due: 31 May 2026
* MediaEval 2026 Workshop, Sat.-Sun. 15-16 June 2026, Amsterdam, Netherlands and Online.


##### The MediaEval Coordination Committee (2026): 
* Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania
* Steven Hicks, SimulaMet, Norway
* Martha Larson, Radboud University, Netherlands

##### MediaEval 2026 is supported by:

<a href="https://www.sigmm.org/">
 <img src="https://multimediaeval.github.io/editions/2020/docs/sigmmlogo.gif" width=150/>
</a><br>
<a href="https://www.adaptcentre.ie/">
  <img src="https://multimediaeval.github.io/editions/2025/docs/adaptlogo.jpg" width=150/>
</a><br>
<a href="https://aicode-project.eu/">
   <img src="https://multimediaeval.github.io/editions/2025/docs/ai-code-1.png" width=150/>
</a>
