<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>No-Audio Multimodal Speech Detection Task | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="No-Audio Multimodal Speech Detection Task" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2020 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2020 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2020/tasks/noaudio-speech/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2020/tasks/noaudio-speech/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-17T05:22:44+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="No-Audio Multimodal Speech Detection Task" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-17T05:22:44+00:00","datePublished":"2025-09-17T05:22:44+00:00","description":"See the MediaEval 2020 webpage for information on how to register and participate.","headline":"No-Audio Multimodal Speech Detection Task","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2020/tasks/noaudio-speech/"},"url":"https://multimediaeval.github.io/editions/2020/tasks/noaudio-speech/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>No-Audio Multimodal Speech Detection Task</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2020/">MediaEval 2020 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task Description</h4>

<p>Task participants are provided with video of individuals participating in a conversation that was captured by an overhead camera. Each individual is also wearing a badge-like device, recording tri-axial acceleration.</p>

<p>The goal of the task is to automatically estimate when the person seen in the video starts speaking, and when they stop speaking using these alternative modalities. In contrast to conventional speech detection, for this task, no audio is used. Instead, the automatic estimation system must exploit the natural human movements that accompany speech (i.e., speaker gestures, as well as shifts in pose and proximity).</p>

<p>This task consists of two subtasks, with a new optional subtask:</p>

<ul>
  <li>
    <p><strong>Unimodal classification:</strong> Design and implement separate speech detection algorithms exploiting each modality separately: Teams must submit separate decisions for the wearable modality and for the video modality.</p>
  </li>
  <li>
    <p><strong>Multimodal classification:</strong> Design and implement a speech detection approach that integrates modalities. Teams must submit a multimodal estimation decision, using some form of early, late or hybrid fusion.</p>
  </li>
  <li>
    <p><strong>Analysis of failure test cases (optional):</strong> From previous editions, test subjects with lower performances compared to the mean have been discovered. In this sub-task, participants are encouraged to analyze these particular subjects and show or hypothesize about possible reasons for such low performances.</p>
  </li>
</ul>

<p>Speaking predictions must be made for every second. However, it is left to the teams if they decide to use a different interval length and later interpolate or extrapolate to the second level.</p>

<h4 id="motivation-and-background">Motivation and Background</h4>

<p>An important but under-explored problem is the automated analysis of conversational dynamics in large unstructured social gatherings such as networking or mingling events. Research has shown that attending such events contributes greatly to career and personal success [7]. While much progress has been made in the analysis of small pre-arranged conversations, scaling up robustly presents a number of fundamentally different challenges.</p>

<p>This task focuses on analysing one of the most basic elements of social behaviour: the detection of speaking turns. Research has shown the benefit of deriving features from speaking turns for estimating many different social constructs such as dominance, or cohesion to name but a few. Unlike traditional tasks that have used audio to do this, here the idea is to leverage the body movements (i.e. gestures) that are performed during speech production which are captured from video and/or wearable acceleration and proximity. The benefit of this is that it enables a more privacy-preserving method of extracting socially relevant information and has the potential to scale to settings where recording audio may be impractical.</p>

<p>The relationship between body behaviour such as gesturing while speaking has been well-documented by social scientists [1]. Some efforts have been made in recent years to try and estimate these behaviours from a single body-worn triaxial accelerometer, hung around the neck [2,3]. This form of sensing could be embedded into a smart ID badge that could be used in settings such as conferences, networking events, or organizational settings. In other works, video has been used to estimate speaking status [4,5]. Despite these efforts, one of the major challenges has been in getting competitive estimation performance compared to audio-based systems. As yet, exploiting the multi-modal aspects of the problem is an under-explored area that will be the main focus of this challenge.</p>

<h4 id="target-group">Target Group</h4>

<p>This challenge is targeted at researchers in wearable devices, computer vision, signal and speech processing. The aim is to provide an entry-level task that has a clearly definable ground truth. There are many nuances to this problem that would enable this problem to be solved better if an intuition behind the behaviour is better understood. The problem could also be solved without this knowledge. The hope, however, is that this task will allow researchers who are not familiar with social signal processing to learn more about the problem domain; we have subsequent challenges in mind in later years that would become increasingly complex in terms of the social context and social constructs that are not so easily understood in terms of their social cue representation (e.g. personality, attraction, conversational involvement). The recommended readings for the challenge are [3,5,6]. Reading references [1,2,4] may provide additional insights on how to solve the problem.</p>

<h4 id="data">Data</h4>

<p>The data consists of 70 people who attended one of three separate mingle events (cocktail parties). Overhead camera data as well as wearable tri-axial accelerometer data for an interval of 30 minutes is available for this task. Each person used a wearable device (to record the acceleration acceleration) hung around the neck as a conference badge. A subset of this data will be kept as a test set. All the samples of this test set will be for subjects who are not in the training set.</p>

<p>All the data is synchronized. The video data is mostly complete, with some segments missing as the participants can leave the recording area at any time (e.g. to go to the bathroom). The frame rate of the video and sample rate of the accelerometer data are captured at 20Hz. Note that due to the crowded nature of the events, there can be strong occlusions between participants in the video, which we hope to evaluate in one of our sub-tasks.</p>

<h4 id="evaluation-methodology">Evaluation Methodology</h4>

<p>Manual annotations are provided for binary speaking status (speaking / non-speaking) for all people. These annotations are carried out for every frame in video (20 FPS). As mentioned above, speaking predictions must be made for every second.</p>

<p>Since the classes are severely imbalanced, we will be using the Area Under the ROC Curve (ROC-AUC) as the evaluation metric. Thus, participants should submit non-binary prediction scores (posterior probabilities, distances to the separating hyperplane, etc.).</p>

<p>The task will be evaluated using a subset of the data left as a test set. All the samples of this test set will be for subjects who are not present in the training set.</p>

<p>For evaluation, we will ask the teams to provide the following estimations for the two subtasks states above (unimodal and multimodal):</p>

<ul>
  <li>
    <p><strong>Person independent:</strong> All samples are provided to the classifier together, irrespective of the subject that the samples came from. Note that the test samples we provide will samples taken from people who are not in the training data.</p>
  </li>
  <li>
    <p><strong>(optional) Person specific:</strong> Only samples generated from the same subject are provided to the classifier. So we expect participants to train one classifier for each person and output test results per person-specific classifier. This can be a useful sanity check as the performance of the method, which should, in theory, perform better when trained on a specific person rather than other people.
The data contains occluded and non-occluded moments, and the performance of systems will be also calculated for both of these subsets individually, in order to gain greater insight into the results that participants achieve.</p>
  </li>
</ul>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>
<!-- # Please use the ACM format for references https://www.acm.org/publications/authors/reference-formatting (but no DOI needed)-->
<!-- # The paper title should be a hyperlink leading to the paper online-->
<p>[1] McNeill, D.: <a href="https://www.cambridge.org/core/books/language-and-gesture/2D216A21B6484304C347FFB0DFCC39BB">Language and gesture</a>, vol. 2. Cambridge University Press (2000)</p>

<p>[2] Hung, H., Englebienne, G., Kools, J.: <a href="https://dl.acm.org/doi/pdf/10.1145/2493432.2493513?casa_token=rB1te_mCb3wAAAAA:dTnFrRm1YqAOVqsixGmJu_Xc1fKZQfhLbuju5meZnMMj1C15xzSQ0yBvnE5Nw3SFnSnXdC9ls3ZEC1M">Classifying social actions with a single accelerometer</a>. In: Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, pp. 207–210. ACM (2013)</p>

<p>[3] Gedik, E. and Hung, H., <a href="https://link.springer.com/article/10.1007/s00779-017-1006-4">Personalised models for speech detection from body movements using transductive parameter transfer</a>, Journal of Personal and Ubiquitous Computing, (2017)</p>

<p>[4] Hung, H. and Ba, S. O., <a href="https://infoscience.epfl.ch/record/146060">Speech/non-speech Detection in Meetings from Automatically Extracted Low Resolution Visual Features</a>, Idiap Research Report, (2010)</p>

<p>[5] Cristani, M., Pesarin, A., Vinciarelli, A., Crocco, M. , and Murino, V., <a href="https://d1wqtxts1xzle7.cloudfront.net/8048683/gestures.pdf?1327801984=&amp;response-content-disposition=inline%3B+filename%3DLook_at_whos_talking_Voice_activity_dete.pdf&amp;Expires=1594664408&amp;Signature=ea8pxw-LIng563aOFzxmlug-7SJqjNvizHJ1UY1kY-ANJ8qq8XS0~EBhOvKVaTT1KgAoducvgJHOdh7md3~jYFqBqcVV7QGsKRt8H5s1Ni0m7yOndhI5Acm6RAJzOUsHCubP3LsyzdClZ5sAP769KLVubpaweNw5uvUJzw8kbOTijVzF7rET4aOmc4FY7m0avFzi4jlYr65kJm5jIG1AOOfY7gycMbhYfJalg4n7C4H2X7Xyt-IqvDfHnpuxSK6Hj4pljfTn8wuFJjt6OTeDmA7jNlyiRMqhpuuvhhoK94N2~Zq1KFe6H4wDGH1BjWSGZwfwkBpL4J3J2BzGJdVCtw__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">Look at who’s talking: Voice activity detection by automated gesture analysis</a>, In the workshop on Interactive Human Behavior Analysis in Open or Public Spaces, International Joint Conference on Ambient Intelligence, (2011).</p>

<p>[6] Cabrera-Quiros, L., Demetriou, A., Gedik, E., van der Meij, L., &amp; Hung, H. (2018). <a href="http://homepage.tudelft.nl/3e2t5/MatchNMingle.pdf">The MatchNMingle dataset: a novel multi-sensor resource for the analysis of social interactions and group dynamics in-the-wild during free-standing conversations and speed dates</a>. IEEE Transactions on Affective Computing.</p>

<p>[7] Wolff, H.-G. and Moser, K. , <a href="http://homepages.se.edu/cvonbergen/files/2013/01/Effects-of-Networking-on-Career-Success_A-Longitudinal-Study.pdf">Effects of networking on career success: a longitudinal study</a>. Journal of Applied Psychology, 94(1):196, (2009).</p>

<h4 id="task-organizers">Task Organizers</h4>
<!-- # add the email address of the contact organizer-->
<ul>
  <li>Laura Cabrera Quiros,  Instituto Tecnológico de Costa Rica, Costa Rica and Delft University of Technology, Netherlands (guest). l dot cabrera at itcr.ac.cr</li>
  <li>Hayley Hung, Delft University of Technology, Netherlands. h dot hung at tudelft dot nl</li>
  <li>Jose Vargas-Quiros, Delft University of Technology, Netherlands. J dot D dot VargasQuiros at tudelft dot nl</li>
</ul>

<!--#### Task Auxiliaries
 # if there are people helping with the task, but are not bearing the main responsibility for the task, they are auxiliaries. Please delete this heading if you have no auxiliaries-->

<h4 id="task-schedule">Task Schedule</h4>
<ul>
  <li>21 July: Data release <!-- # Replace XX with your date. Latest possible is 31 July--></li>
  <li><del>31 October</del> 16 November: Runs due + Start working on Working notes paper <!-- # Replace XX with your date. Latest possible is 31 October--></li>
  <li><del>15 November</del> 23 November: Results returned  <!-- Fixed. Please do not change--></li>
  <li>30 November: Working notes paper  <!-- Fixed. Please do not change--></li>
  <li>11, 14-15 December : MediaEval 2020 Workshop (Fully online.) <!-- Fixed. Please do not change--></li>
</ul>

<p>Workshop will be held online.</p>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
