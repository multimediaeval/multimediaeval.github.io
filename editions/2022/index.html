<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MediaEval 2022 | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="MediaEval 2022" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. Participants address these challenges by creating algorithms for retrieval, analysis, and exploration. Solutions are systematically compared using a common evaluation procedure, making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society." />
<meta property="og:description" content="The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. Participants address these challenges by creating algorithms for retrieval, analysis, and exploration. Solutions are systematically compared using a common evaluation procedure, making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2022/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2022/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-14T13:49:44+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="MediaEval 2022" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-14T13:49:44+00:00","datePublished":"2025-02-14T13:49:44+00:00","description":"The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. Participants address these challenges by creating algorithms for retrieval, analysis, and exploration. Solutions are systematically compared using a common evaluation procedure, making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society.","headline":"MediaEval 2022","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2022/"},"url":"https://multimediaeval.github.io/editions/2022/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>MediaEval 2022</h2>
  </header>

  <p>The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data. Participants address these challenges by creating algorithms for retrieval, analysis, and exploration. Solutions are systematically compared using a common evaluation procedure, making it possible to establish the state of the art and track progress. Our larger aim is to promote reproducible research that makes multimedia a positive force for society.</p>

<p>MediaEval goes beyond other benchmarks and data science challenges in that it also pursues a “Quest for Insight” (Q4I). With Q4I we push beyond only striving to improve evaluation scores to also working to achieve deeper understanding about the challenges. For example, properties of the data,  strengths and weaknesses of particular types of approaches, and observations about the evaluation procedure.</p>

<p>The MediaEval 2022 Workshop will be held 12-13 January 2023, collocated with <a href="https://www.mmm2023.no">MMM 2023</a> in Bergen, Norway and also online. For the preliminary workshop proceedings and the workshop schedule, please see the <a href="https://multimediaeval.github.io/2023/01/06/workshop-information.html">MediaEval Workshop Information Announcement</a>
<!--See the [Workshop Registration Page](https://multimediaeval.github.io/2022/11/16/workshop-registration.html) for more information about the workshop and a link to registration.--></p>

<!--Signup for MediaEval 2022 is now open via the [MediaEval 2022 registration form](https://forms.gle/JcKoa5ycxR2KEiTJ7)-->

<!--Once you are signed up, fill out and return the [MediaEval 2022 usage agreement](https://multimediaeval.github.io/editions/2022/docs/MediaEval2022_UsageAgreement.pdf) to the email address given at the bottom of the first page.-->

<!--The [call for proposals](https://multimediaeval.github.io/) for tasks to run in the 2022 benchmarking season will close on 14 April.-->

<h3 id="workshop">Workshop</h3>

<!-- * Register for the workshop here: [MediaEval 2021 Workshop Registration Form](https://multimediaeval.github.io/editions/2021/docs/MediaEval2021WorkshopScheduleAndThanks.pdf) -->
<ul>
  <li>Workshop schedule is here: <a href="https://multimediaeval.github.io/editions/2022/docs/MultimediaEval_2022_Detailed_Program.pdf">MediaEval 2022 Workshop Program</a></li>
  <li>Proceedings: <a href="https://ceur-ws.org/Vol-3583/">MediaEval 2022 Working Notes Proceedings</a></li>
</ul>

<p>Workshop group photo:</p>

<p><img src="https://multimediaeval.github.io/editions/2022/docs/mediaeval2022GroupPhoto.jpg" width="300" /></p>

<h3 id="task-schedule">Task schedule</h3>

<ul>
  <li>June 2022: Sign up opens for participation in MediaEval 2022</li>
  <li>June-August 2022: Data releases</li>
  <li>Beginning November 2022: Runs due (see task pages for the deadline for each task)</li>
  <li>28 November 2022: Working notes paper due</li>
  <li>12-13 January 2023: MediaEval 2022 Workshop, Collocated with <a href="https://www.mmm2023.no">MMM 2023</a> in Bergen, Norway and also online.</li>
</ul>

<h5 id="the-mediaeval-coordination-committee-2022">The MediaEval Coordination Committee (2022)</h5>
<ul>
  <li>Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania</li>
  <li>Steven Hicks, SimulaMet, Norway</li>
  <li>Martha Larson, Radboud University, Netherlands (Overall coordinator and main contact person)</li>
</ul>

<p>MediaEval is grateful for the support of <a href="http://sigmm.org/">ACM Special Interest Group on Multimedia</a></p>

<p><img src="https://multimediaeval.github.io/editions/2020/docs/sigmmlogo.gif" width="150" /></p>

<!-- ### Task Pre-Announcement

This is a partial list of tasks that MediaEval 2022 will offer. Keep your eye on this page for the list to grow and descriptions to be added soon.

* DisasterMM: Multimedia Analysis of Disaster-Related Social Media Data
* Emotional Mario: A Game Analytics Challenge
* FakeNews: Fake News Detection
* Medico: Medical Multimedia Task: Transparent Tracking of Spermatozoa
* Musti: Multimodal Understanding of Smells in Texts and Images
* NewsImages: Relating news articles and images
* NjordVid: Fishing Trawler Video Analytics Task
* Memorability: Predicting Video Memorability
* Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos
* SwimTrack: Swimmers and Stroke Rate Detection in Elite Race Videos
* Urban Air: Urban Life and Air Pollution
-->

  
  

  
  <h3>
    Task List
  </h3>

  
  <ul class="right hide-on-med-and-down"></ul>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/disastermm/"><h4>DisasterMM: Multimedia Analysis of Disaster-Related Social Media Data</h4></a>
    <p>Contribute to disaster management by addressing two subtasks: Classify multimodal twitter data as relevant or non-relevant to flooding events and and develop a named-entity recognizer in order to identify which words (or sequence of words) in a tweet’s text refer to locations.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/disastermm/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/emotionalmario/"><h4>Emotional Mario: A Game Analytics Challenge</h4></a>
    <p>Identify events of high significance in the Super Mario Bros. gameplay by analyzing facial expressions and the biometric data of players and then (optionally) creating a video summary of the best moments of play.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/emotionalmario/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/fakenews/"><h4>FakeNews Detection</h4></a>
    <p>Participants address three fake news detection subtasks related to COVID-19-related conspiracy theories on twitter: First, text-based topic and conspiracy detection, second, graph based detection of users who post conspiracy theory (posters) in a social network graph with node attributes, and, third, a combination the two to achieve topic and conspiracy detection based both textual data and graphs.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/fakenews/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/musti/"><h4>MUSTI - Multimodal Understanding of Smells in Texts and Images</h4></a>
    <p>Task participants develop classifiers to predict whether a text passage and an image evoke the same smell source or not and (optionally) dectors dentify common smell sources text passages and images.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/musti/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/medico/"><h4>Medical Multimedia Task: Transparent Tracking of Spermatozoa</h4></a>
    <p>Detect and track spermatozoa in medical video, with the goal to create a real-time system. Calculate/predict attributes such as speed and travel distance.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/medico/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/newsimages/"><h4>NewsImages</h4></a>
    <p>Participants are supplied with a large set of articles (including text body, and headlines) and the accompanying images from international publishers. The task requires participants to predict which image was used to accompany each article.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/newsimages/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/njord/"><h4>NjordVid: Fishing Trawler Video Analytics Task</h4></a>
    <p>Task participants are provided with a surveillance video dataset from a fishing trawler. The overall objective of the task is to get more insight into the happenings on fishing trawlers but at the same time keep the privacy of fishing workers as high as possible. The first subtask is to create a method that is able to detect unforeseen events on the boat (anomalies). The seconds subtask is to come up with solutions to protect fishing workers' privacy but at the same time do not influence the automatic analysis of the video streams.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/njord/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/memorability/"><h4>Predicting Video Memorability</h4></a>
    <p>The task requires participants to automatically predict memorability scores for videos, that reflect the probability for a video to be remembered. Participants will be provided with an extensive data set of videos with memorability annotations, related information, pre-extracted state-of-the-art visual features, and Electroencephalography (EEG) recordings.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/memorability/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/sportsvideo/"><h4>Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos.</h4></a>
    <p>This task offers two subtasks: classification of temporally segmented videos of single table tennis strokes and dection of strokes, regardlesss of its class, from untrimmed video.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/sportsvideo/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/swimtrack/"><h4>SwimTrack: Swimmers and Stroke Rate Detection in Elite Race Videos</h4></a>
    <p>The SwimTrack is a series of 5 multimedia tasks related to swimming video analysis from elite competition recordings. These tasks are related to video, image, and audio analysis which may be achieved independently. But when solved together, they form a grand challenge to provide sport federations and coaches with novel methods to assess and enhance swimmers’ performance.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/swimtrack/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2022/tasks/urbanair/"><h4>Urban Air: Urban Life and Air Pollution</h4></a>
    <p>The task requires participants to tackle two subtasks: multimodal/crossmodal air pollution prediction and periodic traffic-pollution patterns discovery. The first requires participants to predict Air Quality Index (AQI) in the short- and mid-term future using multimodal/cross-modal data. Remarkably, the participants must predict AQI using (1) only station data, and (2) station and CCTV data. The second requires participants to discover periodic traffic-pollution patterns that can bring citizens' awareness of traffic-pollution mutual impacts using the given datasets.</p>
    <a href="https://multimediaeval.github.io/editions/2022/tasks/urbanair/">Read more.</a>
    <br>
  
</ul>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
