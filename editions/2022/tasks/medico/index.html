<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Medical Multimedia Task: Transparent Tracking of Spermatozoa | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Medical Multimedia Task: Transparent Tracking of Spermatozoa" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2022 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2022 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2022/tasks/medico/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2022/tasks/medico/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-01T16:39:22+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Medical Multimedia Task: Transparent Tracking of Spermatozoa" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-01T16:39:22+00:00","datePublished":"2025-04-01T16:39:22+00:00","description":"See the MediaEval 2022 webpage for information on how to register and participate.","headline":"Medical Multimedia Task: Transparent Tracking of Spermatozoa","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2022/tasks/medico/"},"url":"https://multimediaeval.github.io/editions/2022/tasks/medico/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Medical Multimedia Task: Transparent Tracking of Spermatozoa</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2022/">MediaEval 2022 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task Description</h4>
<p>The 2022 Medico task tackles the challenge of tracking sperm cells of video recordings of spermatozoa. The development dataset contains 20 videos, each one is 30 seconds long, a set of sperm characteristics (hormones, fatty acids data, etc.), frame-by-frame bounding box annotations, some anonymized study participants-related data, and motility and morphology data following the WHO guidelines. The goal is to encourage task participants to track individual sperms in real-time and combine different data sources to predict common measurements used for sperm quality assessment, specifically the motility (movement) spermatozoa (living sperm).</p>

<p>We hope that this task will encourage the multimedia community to aid in the development of computer-assisted reproductive health and discover new and clever ways of analyzing multimodal datasets. In addition to good analysis performance, an important aspect is also the efficiency of the algorithms due to the fact that the assessment of the sperm is performed in real-time and therefore requires real-time feedback.</p>

<p>For the task, we will provide a dataset of videos and other data from 20 different patients. Based on this data, the participants will be asked to address the following four subtasks:</p>

<ul>
  <li>
    <p><em>Subtask 1: Sperm cell tracking</em> is real-time tracking of sperm cells in a given sperm videos. Tracking should be performed by predicting bounding box coordinates with the similar format to the bounding box coordinates provided with the development datasets. In this task, models should track sperm in each frame of a provided video in real-time. Therefore, frames per second is a important factor to measure.</p>
  </li>
  <li>
    <p><em>Subtask 2: Prediction of motility</em> in terms of the percentage of progressive and non-progressive spermatozoa is the second task. The prediction needs to be performed sample wise resulting in one value per sample per predicted attribute. Sperm tracking or bounding boxes predicted in the task 1 are required to use to solve the task. Motility is the ability of an organism to move independently, and where a progressive spermatozoon is able to “move forward”, a non-progressive would move in circles without any forward progression.</p>
  </li>
  <li>
    <p><em>Subtask 3: Catch and highlight</em> task focus on identifying fastest sperm cells with corresponding average speed and highest top speed. One specific challenge with this subtask is that the video also changes the view on the sample. This happens because the sample is moved below the microscope to observe the complete sample area. Therefore, the tracking has to be performed per viewpoint on the sample. (Optional Subtask.)</p>
  </li>
  <li>
    <p><em>Subtask 4: Explainability of predicitons</em> is perfomed in Subtasks 1 and/or 2 and/or 3 should be explained using machine learning explainable methods to convince domain experts about the final outputs. There is no any specific pre-requirements for this task. However, a report should be provided with explainable methods and corresponding results. (Optional Subtask.)</p>
  </li>
</ul>

<p>For both Subtasks 2 and 3, task-participants are asked to perform video analysis over single frame analysis. This is important due to the fact that single frame-based analysis will not be able to catch the movement of the spermatozoa (motility) which contains important information to perform the predictions on Subtasks 2 and 3.</p>

<h4 id="motivation-and-background">Motivation and background</h4>
<p>Manual evaluation of a sperm sample using a microscope is time-consuming and requires costly experts who have extensive training. In addition, the validity of manual sperm analysis becomes unreliable due to limited reproducibility and high inter-personnel variations due to the complexity of tracking, identifying, and counting sperms in fresh samples. The existing computer-aided sperm analyzer systems are not working well enough for application in a real clinical setting due to unreliability caused by the consistency of the semen sample. Therefore, we need to research new methods for automated sperm analysis.</p>

<h4 id="target-group">Target group</h4>
<p>The task is of interest to researchers in the areas of machine learning (classification), visual content analysis and multimodal fusion. Overall, this task is intended to encourage the multimedia community to help improve the health care system through application of their knowledge and methods to reach the next level of computer and multimedia assisted diagnosis, detection and interpretation.</p>

<h4 id="data">Data</h4>
<p>The task uses the data set VISEM [2], which contains data from 85 male participants aged 18 years or older. For this task, we have selected only 30 seconds video clips from selected 20 videos. For each participant, we include a set of measurements from a standard semen analysis, a video of live spermatozoa, a sperm fatty acid profile, the fatty acid composition of serum phospholipids, study participants-related data, and WHO analysis data. The dataset contains 20 videos, with each video has 30 seconds duration with corresponding bounding box coordinates. Each video has a resolution of 640x480 and runs at 50 frames-per-second. The dataset contains in total six CSV files (five for data and one which maps video IDs to study participants’ IDs), a description file, and  folders containing the videos and bounding box data. The name of each video file contains the video’s ID, the date it was recorded, and a small optional description. Then, the end of the filename contains the code of the person who assessed the video. Furthermore, VISEM contains five CSV files for each of the other data provided, a CSV file with the IDs linked to each video, and a text file containing * descriptions of some of the columns of the CSV files. One row in each CSV file represents a participant. The provided CSV files are:</p>
<ul>
  <li>semen_analysis_data: The results of standard semen analysis.</li>
  <li>fatty_acids_spermatozoa: The levels of several fatty acids in the spermatozoa of the participants.</li>
  <li>fatty_acids_serum: The serum levels of the fatty acids of the phospholipids (measured from the blood of the participant).</li>
  <li>sex_hormones: The serum levels of sex hormones measured in the blood of the participants.</li>
  <li>study_participant_related_data: General information about the participants such as age, abstinence time, and Body Mass Index (BMI).</li>
  <li>videos: Overview of which video file belongs to what participant.</li>
</ul>

<p>All Study participants agreed to donate their data for the purpose of science and provided the necessary consent for us to be able to distribute the data (checked and approved by the Norwegian data authority and ethical committee).</p>

<h5 id="development-data-is-available-now">Development data is available now:</h5>
<ul>
  <li><a href="https://www.kaggle.com/datasets/vlbthambawita/visemtracking">Data download link 1 - Kaggle</a></li>
  <li><a href="https://datasets.simula.no/visem-tracking/">Data download link 2 - Simula-dataset</a></li>
</ul>

<h4 id="ground-truth">Ground truth</h4>
<p>The ground truth data provided in this task were prepared by expert computer scientists and verified by domain experts.</p>

<h4 id="evaluation-methodology">Evaluation methodology</h4>
<p>For the evaluation, we will use mAP (mean average precision), mean squared error, mean absolute error, frames per seconds and the mean absolute percentage error for the first two subtasks. For the optional third and fourth task, we will use manual evaluation with the help of three different experts within human reproduction.</p>

<h3 id="test-data-downlod-link">Test data downlod link</h3>
<p>The prediction of this test dataset should be uploaded using the following submission form.
<a href="https://www.dropbox.com/sh/2ohitza5ouzh2d3/AAD_8VnvdhPqOVlCcAn21Uc8a?dl=0">Test data download link</a></p>

<h3 id="submission-instructions">Submission instructions</h3>

<p><a href="https://forms.gle/Bgwt5pEwwKm6HPH26">Submission form</a></p>

<h4 id="sub-task-1">Sub-task 1:</h4>

<p>If you are interested in submitting only for detecting sperm in individual frames, then your submission file should be matched to the provided ground truth format (YOLO format). You have to follow the similar file structure of the dataset. Check the folder structure in <a href="https://www.kaggle.com/datasets/vlbthambawita/visemtracking">https://www.kaggle.com/datasets/vlbthambawita/visemtracking</a>. A sample .txt file is below.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source_code
   |- code_and_checkpoints
	  |- README.txt (must explain how to run your model to detect sperms on a new video)
	  |- run.sh (shell script file to run your models for new video inputs (.mp4))
predictions
   |- &lt;test_video_ id&gt;
	|- labels
             |- &lt;video id&gt;_frame_0.txt
             |- &lt;video id&gt;_frame_1.txt
             |- &lt;video id&gt;_frame_2.txt
              ...
 
        |-labels_ftid (optional) # labels with unique feature IDs to track them via multiple frames
             |- &lt;video id&gt;_frame_0.txt with tracking IDs.
             |- &lt;video id&gt;_frame_1.txt with tracking IDs.
             |- &lt;video id&gt;_frame_2.txt with tracking IDs.
             ...
        |- &lt;video id&gt;.mp4 (showing sperm detection information)
        |- &lt;video id&gt;_tracking.mp4 (showing sperm tracking information) - optional
	  |- ...
	
</code></pre></div></div>

<h4 id="sub-task-2">Sub-task 2:</h4>

<p>For subtask 2, we will compare your results with a ground truth file similar to semen_analysis_data_Train.csv. So, you have to predict progressive motility (%), Non progressive sperm motility (%) and Immotile sperm (%). Check the CSV file for these columns. The sum of these three values is 100%.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>– source_code
	|– code_and_checkpoints
	|– README.txt (must explain how to run your model to predict motility level of a new video)
	|– run.sh (shell script file to run your models for new video inputs (.mp4)) # must work with test video files

– motility_predictions.csv

--------------
Sample format
--------------
ID, Progressive motility (%), Non progressive sperm motility (%), Immotile sperm (%)
1, 25, 75, 25
2, 45, 35, 20
…
</code></pre></div></div>

<h4 id="sub-task-3-optional">Sub-task 3 (Optional):</h4>
<p>In this task, you have to highlight the fastest sperms and detect and track them within a view point of a given video.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>– source_code
	|– code_and_checkpoints
	|– README.txt (must explain how to run your model to predict fastest sperms in a given video)
	|– run.sh (shell script file to run your models for new video inputs (.mp4))

– predictions
	|– &lt;test_video_ id_1&gt;
	  |– labels # containing bounding box detail of fastest sperms and sperm tracking IDs
		   |– &lt;video id&gt;_frame_0.txt
     |– &lt;video id&gt;_frame_1.txt
     |– &lt;video id&gt;_frame_2.txt
      ...
 
	|– &lt;test_video_id_2&gt;
		 ...
 |-fastest_sperms.csv
 
  # columns of the fastest_sperms.csv
		Video_ID, view_point_start_frame_id, view_point_stop_frame, fastest_sperm_tracking_ID, speed

	|-&lt;video_id&gt;.mp4 # with the highlighted fastest sperm(s) and corresponding details
 ...
</code></pre></div></div>

<h4 id="sub-task-4-optional">Sub-task 4 (Optional):</h4>

<p>You can upload PDF files, Jupiter notebooks, or/and video files explaining your approaches and results of Sub-task 4. The submission of Sub-task 4 will be evaluated manually.</p>

<h4 id="quest-for-insight">Quest for insight</h4>
<p>Here are several research questions related to this challenge that participants can strive to answer in order to go beyond just looking at the evaluation metrics:</p>
<ul>
  <li>Will continued tracking of sperm help to analyze the motility level of sperm samples?</li>
  <li>How do we calculate the average speed of moving sperms, and how to track the fastest one among many moving sperms?</li>
  <li>How can we best communicate to doctors about the accuracy, reliability, and trustworthiness of the output of Deep Learning methods?</li>
</ul>

<h4 id="participant-information">Participant information</h4>
<p>Please contact your task organizers with any questions on these points.</p>
<ul>
  <li>Signing up: Fill in the <a href="https://forms.gle/JcKoa5ycxR2KEiTJ7">registration form</a> and fill out and return the <a href="https://multimediaeval.github.io/editions/2022/docs/MediaEval2022_UsageAgreement.pdf">usage agreement</a>.</li>
  <li>Making your submission: To be announced (check the task read me) <!-- Please add instructions on how to create and submit runs to your task replacing "To be announced." --></li>
  <li>Preparing your working notes paper: Instructions on preparing you working notes paper can be found in <a href="https://docs.google.com/document/d/12uSn0rRYxa3buiFNEbpa46dKsHOyqV2PHU_joRGMHRw">MediaEval 2022 Working Notes Paper Instructions</a>.</li>
</ul>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>
<p>[1] Riegler, Michael, et al. “Multimedia and Medicine: Teammates for Better Disease Detection and Survival.” Proceedings of the 2016 ACM on Multimedia Conference. ACM, 2016.</p>

<p>[2] Trine B. Haugen, Steven A. Hicks, Jorunn M. Andersen, Oliwia Witczak, Hugo L. Hammer, Rune Borgli, Pål Halvorsen, and Michael Riegler. 2019. VISEM: a multimodal video dataset of human spermatozoa. In Proceedings of the 10th ACM Multimedia Systems Conference (MMSys ‘19). Association for Computing Machinery, New York, NY, USA, 261–266. DOI:https://doi.org/10.1145/3304109.3325814</p>

<p>[3] Hicks, S.A., Andersen, J.M., Witczak, O. et al. Machine Learning-Based Analysis of Sperm Videos and Participant Data for Male Fertility Prediction. Sci Rep 9, 16770 (2019). https://doi.org/10.1038/s41598-019-53217-y</p>

<p>[4] Thambawita, V., Halvorsen, P., Hammer, H., Riegler, M., &amp; Haugen, T. B. (2019). Stacked dense optical flows and dropout layers to predict sperm motility and morphology. arXiv preprint arXiv:1911.03086.</p>

<p>[5] Thambawita, V., Halvorsen, P., Hammer, H., Riegler, M., &amp; Haugen, T. B. (2019). Extracting temporal features into a spatial domain using autoencoders for sperm video analysis. arXiv preprint arXiv:1911.03100.</p>

<h4 id="task-organizers">Task organizers</h4>
<ul>
  <li>Vajira Thambawita, vajira@simula.no, SimuaMet</li>
  <li>Steven Hicks, steven@simula.no, SimulaMet</li>
  <li>Andrea Storås , andrea@simula.no, SimulaMet</li>
  <li>Hugo Lewi Hammer, hugo@simula.no, SimulaMet</li>
  <li>Michael Riegler, michael@simula.no, SimulaMet</li>
  <li>Pål Halvorsen, paalh@simula.no, SimulaMet</li>
</ul>

<h4 id="task-schedule">Task Schedule</h4>
<ul>
  <li>1 August 2022: Data release.</li>
  <li>1 November 2022: Runs due and results returned.</li>
  <li>20 November 2022: Results returned.  <!--Latest possible should be 23 November--></li>
  <li>28 November 2022: Working notes paper.  <!-- Fixed. Please do not change.--></li>
  <li>12-13 January 2023: 13th Annual MediaEval Workshop, Collocated with <a href="https://www.mmm2023.no/">MMM 2023</a> in Bergen, Norway and also online. <!-- Fixed. Please do not change.--></li>
</ul>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
