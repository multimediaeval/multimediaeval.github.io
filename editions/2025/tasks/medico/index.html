<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Medico 2025: Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Medico 2025: Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2025 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2025 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2025/tasks/medico/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2025/tasks/medico/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-17T05:03:36+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Medico 2025: Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-17T05:03:36+00:00","datePublished":"2025-09-17T05:03:36+00:00","description":"See the MediaEval 2025 webpage for information on how to register and participate.","headline":"Medico 2025: Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2025/tasks/medico/"},"url":"https://multimediaeval.github.io/editions/2025/tasks/medico/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Medico 2025: Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2025/">MediaEval 2025 webpage</a> for information on how to register and participate.</em></p>

<p><em>See our <a href="https://github.com/simula/MediaEval-Medico-2025">GitHub Repository</a> for the latest information about the task. We encourage participants to check the repository regularly for updates.</em></p>

<h4 id="task-description">Task description</h4>

<p>Gastrointestinal (GI) diseases are among the most common and critical health concerns worldwide, with conditions like colorectal cancer (CRC) requiring early diagnosis and intervention. AI-driven decision support systems have shown potential in assisting clinicians with diagnosis, but a major challenge remains: explainability. While deep learning models can achieve high diagnostic accuracy, their “black-box” nature limits their adoption in clinical practice, where trust and interpretability are essential. After successfully organizing multiple Medico challenges at MediaEval in previous years, we propose a new task for Medico 2025: <strong>Visual Question Answering (with multimodal explanations) for Gastrointestinal Imaging</strong>.</p>

<p>Medical Visual Question Answering (VQA) is a rapidly growing research area that combines computer vision and natural language processing to answer clinically relevant questions based on medical images. However, existing VQA models often lack transparency, making it difficult for healthcare professionals to assess the reliability of AI-generated answers. To address this, the Medico 2025 challenge will focus on explainable VQA for GI imaging, encouraging participants to develop models that provide not only accurate answers but also clear justifications aligned with clinical reasoning.</p>

<p>This challenge will offer a benchmark dataset containing GI images, videos, and associated VQA annotations, allowing for rigorous evaluation of AI models. By integrating multimodal data and explainability metrics, we aim to advance research in interpretable AI and improve the potential for clinical adoption.</p>

<p>We define two main subtasks for this year’s challenge. Subtask 2 builds on Subtask 1, meaning Subtask 1 must be completed in order to participate in Subtask 2.</p>

<ul>
  <li>
    <p><strong>Subtask 1: AI Performance on Medical Image Question Answering</strong><br />
This subtask challenges participants to develop AI models that can accurately interpret and respond to clinical questions based on GI images from the <strong>Kvasir-VQA-x1</strong> dataset, which contains 159,549 question–answer pairs from 6,500 original GI images, with additional weakly augmented images and complexity-level annotations. Questions fall into six main categories: Yes/No, Single-Choice, Multiple-Choice, Color-Related, Location-Related, and Numerical Count, as well as merged reasoning-based questions. Performance will be assessed using metrics such as BLEU, ROUGE (1/2/L), and METEOR, alongside medical correctness and relevance.</p>
  </li>
  <li>
    <p><strong>Subtask 2: Clinician-Oriented Multimodal Explanations in GI</strong><br />
This subtask extends Subtask 1 by focusing on the interpretability of model outputs for clinical decision-making. Models must not only generate accurate answers but also provide clear, multimodal explanations that enhance clinician trust and usability. Explanations must be faithful to the model’s reasoning, clinically relevant, and useful for real-world decision-making. Participants are encouraged to combine textual clinical reasoning with visual localization (e.g., heatmaps, segmentation masks, bounding boxes) and/or confidence measures. Performance will be assessed based on answer correctness, explanation clarity, visual alignment, confidence calibration, and medical relevance, as rated by expert reviewers.</p>
  </li>
</ul>

<h4 id="motivation-and-background">Motivation and background</h4>

<p>Medical AI systems must be both accurate and interpretable to be useful in clinical practice. While deep learning models have shown great potential in diagnosing gastrointestinal (GI) conditions from medical images, their adoption remains limited due to a lack of transparency. Clinicians need to understand why an AI system makes a specific decision, especially when it comes to critical medical diagnoses. Explainable AI (XAI) methods aim to bridge this gap by providing justifications that align with clinical reasoning, improving trust, reliability, and ultimately patient outcomes.</p>

<p>This challenge builds upon previous work in medical VQA, where AI models answer clinically relevant questions based on GI images. However, traditional VQA models often provide answers without explanations, making it difficult for medical professionals to assess their validity. By incorporating explainability into the task, we encourage the development of models that not only provide accurate responses but also offer meaningful insights into their decision-making process. This will help ensure that AI systems can be safely integrated into clinical workflows, assisting rather than replacing human expertise.</p>

<h4 id="target-group">Target group</h4>

<p>We invite participation from multiple communities, including computer vision, natural language processing, multimedia analysis, medical imaging, and human–AI interaction. We strongly believe that many multimedia researchers can contribute to this medical scenario, and we hope that many people will be personally motivated to take on the challenge and try out their ideas. To ensure that young researchers succeed, we will also provide mentoring for students at both undergraduate and graduate levels.</p>

<h4 id="data">Data</h4>

<p>The dataset for Medico 2025, <strong>Kvasir-VQA-x1</strong> [1, 2], is a large-scale text–image pair gastrointestinal (GI) dataset built upon the HyperKvasir and Kvasir-Instrument datasets, now enhanced with 159,549 naturalized question–answer annotations, complexity-level scores for curriculum training, and weak augmentations (10 per original image). It is specifically designed to support Visual Question Answering (VQA) and other multimodal AI applications in GI diagnostics.</p>

<p>Dataset: <a href="https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1">https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1</a></p>

<h4 id="evaluation-methodology">Evaluation methodology</h4>

<p><strong>Subtask 1: VQA Performance</strong></p>
<ul>
  <li>Metrics: BLEU, ROUGE (1/2/L), METEOR</li>
  <li>Settings: Original &amp; augmented images</li>
  <li>Criteria: Accuracy, relevance, and medical correctness</li>
</ul>

<p><strong>Subtask 2: Explainability</strong></p>
<ul>
  <li>Metrics: All Subtask 1 metrics</li>
  <li>Expert-rated on:
    <ul>
      <li>Answer correctness</li>
      <li>Clarity &amp; clinical relevance</li>
      <li>Visual alignment</li>
      <li>Confidence calibration</li>
      <li>Methodology &amp; novelty</li>
    </ul>
  </li>
</ul>

<h4 id="quest-for-insight">Quest for insight</h4>

<p>Here are several research questions participants can strive to answer:</p>
<ul>
  <li>Which types of explanations align best with clinical reasoning and enhance trust among medical professionals?</li>
  <li>How can visual attention mechanisms, uncertainty estimation, or multimodal reasoning be leveraged to provide meaningful justifications?</li>
  <li>How can preprocessing and post-processing techniques be optimized to improve explainability while maintaining accuracy?</li>
  <li>What are the most effective strategies for evaluating the quality and reliability of AI-generated explanations in GI diagnostics?</li>
</ul>

<h4 id="participant-information">Participant information</h4>
<p>More details will follow on the competition repository. Please check it regularly: <a href="https://github.com/simula/MediaEval-Medico-2025">https://github.com/simula/MediaEval-Medico-2025</a></p>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>

<p><em>References</em></p>
<ul>
  <li>[1] Sushant Gautam, Andrea Storås, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, Pål Halvorsen, Michael A. Riegler, <a href="https://arxiv.org/abs/2409.01437">Kvasir-VQA: A Text-Image Pair GI Tract Dataset</a></li>
  <li>[2] Borgli, H., Thambawita, V., Smedsrud, P.H. et al. <a href="https://www.nature.com/articles/s41597-020-00622-y">HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</a></li>
  <li>[3] Hicks, S.A., Strümke, I., Thambawita, V. et al. <a href="https://www.nature.com/articles/s41598-022-09954-8">On evaluation metrics for medical applications of artificial intelligence</a></li>
</ul>

<p><em>Recommended</em></p>
<ul>
  <li><a href="https://visualqa.org/">visualqa</a></li>
  <li><a href="https://www.imageclef.org/2023/medical/vqa">ImageCLEFmed MEDVQA-GI</a></li>
</ul>

<h4 id="task-organizers">Task organizers</h4>
<ul>
  <li>Sushant Gautam, SimulaMet, Norway</li>
  <li>Vajira Thambawita, SimulaMet, Norway</li>
  <li>Pål Halvorsen, SimulaMet, Norway</li>
  <li>Michael A. Riegler, SimulaMet, Norway</li>
  <li>Steven A. Hicks, SimulaMet, Norway (steven@simula.no)</li>
</ul>

<h4 id="task-schedule">Task schedule</h4>
<p>The program will be updated with the exact dates.</p>

<ul>
  <li>14 May 2025: Development Data release</li>
  <li>1 September 2025: Testing Data release</li>
  <li>12 September 2025: Runs due</li>
  <li>26 September 2025: Results returned</li>
  <li>08 October 2025: Working notes paper due</li>
  <li>25–26 October 2025: MediaEval Workshop, Dublin, Ireland and Online</li>
</ul>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
