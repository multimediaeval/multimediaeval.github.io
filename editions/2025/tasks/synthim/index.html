<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Synthetic Images: Advancing detection of generative AI used in real-world online images | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Synthetic Images: Advancing detection of generative AI used in real-world online images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2025 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2025 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2025/tasks/synthim/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2025/tasks/synthim/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-25T06:14:50+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Synthetic Images: Advancing detection of generative AI used in real-world online images" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-25T06:14:50+00:00","datePublished":"2025-04-25T06:14:50+00:00","description":"See the MediaEval 2025 webpage for information on how to register and participate.","headline":"Synthetic Images: Advancing detection of generative AI used in real-world online images","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2025/tasks/synthim/"},"url":"https://multimediaeval.github.io/editions/2025/tasks/synthim/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2025/" style="color:white;">MediaEval 2025</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Synthetic Images: Advancing detection of generative AI used in real-world online images</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2025/">MediaEval 2025 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task description</h4>

<p>The goal of this challenge is to develop AI models capable of detecting synthetic images and identifying the specific regions in the images that have been manipulated or synthesized. Participants are expected to engage in two subtasks:</p>
<ul>
  <li><strong>Synthetic Image Detection:</strong> Build models that classify images as either real or synthetic (binary).</li>
  <li><strong>Manipulated Region Localization:</strong> Extend their models to localize the specific regions within synthetic images that have been generated or modified.</li>
</ul>

<p>In addition to classifying images and localizing manipulations, the robustness of the models will be also evaluated by assessing their ability to identify synthetic images and manipulated regions that have undergone real-world transformations, such as compression, resizing, and cropping, commonly found on social media. Robust and accurate detection and localization under these conditions are critical for success.</p>

<p>Participants will use the provided training and testing datasets and submit their trained models and results for evaluation on a hidden test set. There will also be an open phase where participants are allowed to use their own resources or any publicly available dataset for training [7], in addition to the training data provided by the organizers. However, in such cases, participants must provide detailed information about the resources they use to ensure transparency and address any potential data leakage issues.</p>

<h4 id="motivation-and-background">Motivation and background</h4>

<p>With the proliferation of generative AI technologies, synthetic media is increasingly used in creative industries, social media, and, alarmingly, in misinformation campaigns. Synthetic image detection is vital for combating the misuse of AI-generated content, ensuring trust in visual media, and upholding ethical standards in digital communication. While recent advancements in synthetic image classification have shown promise, challenges persist due to evolving generative models and post-processing techniques such as image recompression, that can obfuscate synthetic traces.</p>

<p>This challenge aims to bring together researchers and practitioners to push the boundaries of synthetic image detection. By focusing on both fully synthetic and partially modified images, this challenge highlights the nuanced nature of synthetic content in real-world scenarios and seeks solutions that are robust, generalizable, and explainable.</p>

<h4 id="target-group">Target group</h4>

<p>The challenge is aimed at researchers and practitioners in computer vision, machine learning, media forensics, and AI ethics who are interested in synthetic image detection, image manipulation detection and generative model attribution. It appeals to individuals working on robust and generalizable solutions for real-world scenarios, including academic teams, industry R&amp;D groups, and independent AI enthusiasts.</p>

<h4 id="data">Data</h4>

<p>The challenge dataset will consist of three sets: a training set, a validation set to be used during development, and a test set without labels, which will be used by the organizers to assess the performance of submitted approaches. These datasets will contain real, synthetic, and partially modified images to comprehensively address the challenge tasks.</p>

<p>The training and validation datasets will include a combination of well-known synthetic image detection (SID) datasets, along with newly generated synthetic content tailored for the task. Additionally, real-world examples collected from social media platforms will be included into the validation dataset to ensure “in-the-wild” evaluation, reflecting diverse real-world transformations and scenarios.</p>
<ul>
  <li><strong>Real Images:</strong> Sourced from established datasets and open-license repositories, such as LAION dataset and RAISE dataset, to ensure high quality, diversity and authenticity.</li>
  <li><strong>Synthetic Images:</strong> Generated using a wide range of generative models, from earlier GAN-based approaches (e.g., StyleGAN2, ProGAN, GigaGAN) to more advanced diffusion models (Stable Diffusion). Synthetic images from commercial platforms like MidJourney, DALL·E 3 and Adobe Firefly will also be included. This set also includes images that have undergone substantial transformations.</li>
  <li><strong>Partially Modified Images [3]:</strong> Approximately 75,000 manipulated images and localization masks, generated using text-guided inpainting methods, including Stable Diffusion v2, Stable Diffusion XL, and Adobe Firefly. The dataset includes two types of manipulated images: images, where an inpainted region is seamlessly integrated into the original image, and fully regenerated images, where the entire image is synthetically regenerated despite only a specific region being manipulated.</li>
  <li><strong>Real-world variations:</strong> A small set of synthetic images has been collected from social media and the Internet, representing variations that have undergone heavy post-processing multiple times, such as compression, resizing, cropping, and other transformations. All images in this set are synthetic, but they reflect real-world alterations commonly found on digital platforms, ensuring a robust evaluation of model performance under realistic conditions [2].</li>
</ul>

<p>All data will be curated under open-source or permissive licenses to ensure ethical use and compliance with data-sharing guidelines.</p>

<h4 id="evaluation-methodology---real-vs-synthetic-task-binary-classification">Evaluation methodology - Real vs. Synthetic Task (Binary Classification)</h4>

<p>For the evaluation of synthetic image detection, the metrics used by the SIDBench framework [1] will be employed to assess performance in depth.</p>

<p>Accuracy: Percentage of correctly classified images.
Precision, Recall, and F1-Score: Evaluate trade-offs between false positives and false negatives, with the F1-Score as a primary indicator of balanced performance.
ROC AUC (Area Under the ROC Curve): Summarizes the trade-off between true positive rate (sensitivity) and the false-positive rate across thresholds.
Average Precision (AP): Evaluates the precision-recall trade-off across thresholds.
Equal Error Rate (EER): The rate at which false acceptance and false rejection are equal, indicating threshold-independent performance.</p>

<p>Τhe F1-Score will be used as the primary metric for ranking participants. This ensures a balanced evaluation of precision and recall, which is critical for synthetic image detection tasks.</p>

<p>To evaluate model robustness in detecting synthetic images under uncontrolled conditions, such as transformations applied by online platforms, we will test the submitted models on the dataset of images collected from social media previously used in disinformation campaigns. The variations, collected directly from the internet, reflect real-world, black-box transformations where the exact processes are unknown. The evaluation will focus on calculating the True Positive Rate (TPR) to measure detection effectiveness across all variations.</p>

<h4 id="evaluation-methodology---manipulated-region-localization-task">Evaluation methodology - Manipulated Region Localization Task</h4>

<p>For this subtask, we first evaluate whether the model correctly identifies an image as manipulated or not. The same metrics as the binary classification will be used, with F1 being the metrics used for ranking. To evaluate how well the model identifies the specific regions in an image that have been manipulated the Intersection over Union (IoU) will be used. This metrics measures the overlap between the predicted manipulated region and the ground truth region: <em>IoU = Area of Overlap / Area of Union</em></p>

<h4 id="quest-for-insight">Quest for insight</h4>
<p>Here are several research questions related to this challenge that participants can strive to answer in order to go beyond just looking at the evaluation metrics:</p>
<ul>
  <li>Are the synthetic images and image regions that are challenging to detect automatically also challenging for people looking at the images to see?</li>
  <li>What are the characteristics of the false positives, i.e., of non-synthetic images or image regions that are misclassified as synthetic? Are there any generalizations in terms of perceived quality or image semantics?</li>
  <li>Are there characteristics (semantic content, framing) that are specific to the in-the-wild examples?</li>
</ul>

<h4 id="participant-information">Participant information</h4>
<!-- Please contact your task organizers with any questions on these points. -->
<!-- # * Signing up: Fill in the [registration form]() and fill out and return the [usage agreement](). -->
<!-- # * Making your submission: To be announced (check the task read me) <!-- Please add instructions on how to create and submit runs to your task replacing "To be announced." -->
<!-- # * Preparing your working notes paper: Instructions on preparing you working notes paper can be found in [MediaEval 2023 Working Notes Paper Instructions]().-->
<p>More details will follow.</p>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>
<p>[1] Schinas, M., &amp; Papadopoulos, S. (2024, June). SIDBench: A Python framework for reliably assessing synthetic image detection methods. In Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation (pp. 55-64).</p>

<p>[2] Karageorgiou, D., Bammey, Q., Porcellini, V., Goupil, B., Teyssou, D., &amp; Papadopoulos, S. (2024, September). Evolution of Detection Performance throughout the Online Lifespan of Synthetic Images. In Trust What You learN (TWYN) Workshop ECCV 2024.</p>

<p>[3] Mareen, H., Karageorgiou, D., Van Wallendael, G., Lambert, P., &amp; Papadopoulos, S. (2024, December). TGIF: Text-Guided Inpainting Forgery Dataset. In 2024 IEEE International Workshop on Information Forensics and Security (WIFS) (pp. 1-6).</p>

<p>[4] Koutlis, C., &amp; Papadopoulos, S. (2025). Leveraging representations from intermediate encoder-blocks for synthetic image detection. In European Conference on Computer Vision (pp. 394-411). Springer, Cham.</p>

<p>[5] Konstantinidou, D., Koutlis, C., &amp; Papadopoulos, S. (2024). TextureCrop: Enhancing cbSynthetic Image Detection through Texture-based Cropping. arXiv preprint arXiv:2407.15500.</p>

<p>[6] Karageorgiou, D., Papadopoulos, S., Kompatsiaris, I., &amp; Gavves, E. (2024). Any-Resolution AI-Generated Image Detection by Spectral Learning. arXiv preprint arXiv:2411.19417.</p>

<p>[7] Bammey, Q. (2024). Synthbuster: Towards Detection of Diffusion Model Generated Images. IEEE Open Journal of Signal Processing, 5, 1-9.</p>

<h4 id="task-organizers">Task organizers</h4>
<ul>
  <li>Manos Schinas, MeVer group, CERTH-ITI, Greece</li>
  <li>Dimitrios Karageogiou, MeVer group, CERTH-ITI, Greece</li>
  <li>Despina Konstantinidou, MeVer group, CERTH-ITI, Greece</li>
  <li>Olga Papadopoulou, MeVer group, CERTH-ITI, Greece</li>
  <li>Symeon Papadopoulos, MeVer group, CERTH-ITI, Greece</li>
  <li>Christos Koutlis, MeVer group, CERTH-ITI, Greece</li>
  <li>Hannes Mareen, IDLab-MEDIA, Univ. Ghent, Belgium</li>
  <li>Efstratios Gavves, VIS Lab, UvA, Netherlands</li>
  <li>Luisa Verdoliva, GRIP, Univ. Naples Federico II, Italy</li>
  <li>Davide Cozzolino, GRIP, Univ. Naples Federico II, Italy</li>
  <li>Fabrizio Guillaro, GRIP, Univ. Naples Federico II, Italy</li>
</ul>

<h4 id="task-schedule">Task schedule</h4>
<p>The program will be updated with the exact dates.</p>

<ul>
  <li>May 2025: Development Data release</li>
  <li>June 2025: Development Data release</li>
  <li>September 2025: Runs due and results returned. Exact dates to be announced.</li>
  <li>08 October 2025: Working notes paper</li>
  <li>25-26 October 2025: MediaEval Workshop, Dublin, Ireland and Online..</li>
</ul>

<h4 id="acknowledgements">Acknowledgements</h4>
<p>The task organization is supported by the Horizon Europe AI-CODE and vera.ai projects that focus on the development of AI tools for supporting media professionals in their verification and fact-checking activities.</p>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
