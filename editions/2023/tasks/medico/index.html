<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Medical Multimedia Task - Transparent Tracking of Spermatozoa | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Medical Multimedia Task - Transparent Tracking of Spermatozoa" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2023 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2023 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2023/tasks/medico/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2023/tasks/medico/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-16T14:25:49+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Medical Multimedia Task - Transparent Tracking of Spermatozoa" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-01-16T14:25:49+00:00","datePublished":"2026-01-16T14:25:49+00:00","description":"See the MediaEval 2023 webpage for information on how to register and participate.","headline":"Medical Multimedia Task - Transparent Tracking of Spermatozoa","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2023/tasks/medico/"},"url":"https://multimediaeval.github.io/editions/2023/tasks/medico/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Medical Multimedia Task - Transparent Tracking of Spermatozoa</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2023/">MediaEval 2023 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task description</h4>
<p>The 2023 Medico task tackles the challenge of tracking sperm cells in video recordings of spermatozoa. The provided development dataset contains 20 videos which have frame-by-frame bounding box annotations, each one 30 seconds long, as well as a set of sperm characteristics (hormone levels, fatty acid data, etc.), anonymized study participant data, and motility and morphology data aligned with WHO guidelines. The goal is to inspire task participants to track individual sperm cells in real time and integrate various data sources to predict standard measurements used for sperm quality assessment, specifically the motility (movement) of spermatozoa (living sperm cells).</p>

<p>We hope that this task will motivate the multimedia community to assist in the advancement of computer-assisted reproductive health and to devise innovative methods for analyzing multimodal datasets. In addition to effective analysis, the efficiency of the algorithms is crucial due to the real-time nature of the sperm assessment, which necessitates immediate feedback.</p>

<p>For the task, we will provide a dataset of videos and other data from 20 different patients. Based on this data, the participants will be asked to solve the following three subtasks:</p>

<ol>
  <li>Sperm detection and tracking : This task aims to automatically localize and track all sperm cells in a given video. In order to develop medical applications in the real world, this task focuses on both the prediction accuracy and efficiency (i.e., processing time) of proposed solutions. Particularly, for a given video of microscopic sperm examinations in which sperms are manually annotated by experts, participants are required to detect sperm cells on all video frames and track them. Tracking should be performed by predicting bounding box coordinates with the similar format to the bounding box coordinates provided with the development datasets.</li>
  <li>Efficient detection and tracking: This task is very similar to Task 1 but has a larger focus on the efficiency of the system and not just the final predictions. Therefore, frames per second is an important factor to measure. To evaluate the efficiency of solutions, participants also need to report FPS and FLOPS of batch size of 1 during the inference state. To achieve the goal, participants expectedly develop methods with high prediction accuracy and fast inference times.</li>
  <li>Prediction of motility: in terms of the percentage of progressive and non-progressive spermatozoa is the second task. The prediction needs to be performed sample wise resulting in one value per sample per predicted attribute. Sperm tracking or bounding boxes predicted in task 1 are required to use to solve the task. Motility is the ability of an organism to move independently, and where a progressive spermatozoon is able to “move forward”, a non-progressive would move in circles without any forward progression.</li>
  <li>(Experimental) Predicting motility using graph data structures: We provide graph data extracted from the original VISEM-Tracking dataset. In this task, we ask participants to use these graph data structures as input to a model to predict motility level of sperm samples.</li>
</ol>

<h4 id="motivation-and-background">Motivation and background</h4>
<p>Manual evaluation of a sperm sample using a microscope is time-consuming and requires costly experts who have extensive training. In addition, the validity of manual sperm analysis becomes unreliable due to limited reproducibility and high inter-personnel variations due to the complexity of tracking, identifying, and counting sperms in fresh samples. The existing computer-aided sperm analyzer systems are not working well enough for application in a real clinical setting due to unreliability caused by the consistency of the semen sample. Therefore, we need to research new methods for automated sperm analysis.</p>

<h4 id="target-group">Target group</h4>
<p>Through our broad team, we can actively invite people from multiple communities to submit solutions to the proposed task. We strongly believe that a significant fraction of multimedia researchers can contribute to the medical scenario. Therefore, we hope that many people are interested and involved on a personal level supporting a decision to work on the task and try out their ideas. To ensure that young researchers succeed, we will also provide mentoring for students that want to tackle the task (undergraduate and graduate levels are very welcome).</p>

<h4 id="data">Data</h4>
<p>VISEM [2] contains data from 85 male participants aged 18 years or older. For this task, we have selected only 30 seconds video clips from selected 20 videos.  For each participant, we include a set of measurements from a standard semen analysis, a video of live spermatozoa, a sperm fatty acid profile, the fatty acid composition of serum phospholipids, study participants-related data, and WHO analysis data. Every frame of videos have the corresponding bounding box coordinates of sperms. Each video has a resolution of 640x480 and runs at 50 frames-per-second. The dataset contains in total six CSV files (five for data and one which maps video IDs to study participants’ IDs), a description file, and  folders containing the videos and bounding box data. The name of each video file contains the video’s ID, the date it was recorded, and a small optional description. Then, the end of the filename contains the code of the person who assessed the video. Furthermore, VISEM contains five CSV files for each of the other data provided, a CSV file with the IDs linked to each video, and a text file containing descriptions of some of the columns of the CSV files. One row in each CSV file represents a participant. The provided CSV files are:</p>

<ul>
  <li>semen_analysis_data: The results of standard semen analysis.</li>
  <li>fatty_acids_spermatozoa: The levels of several fatty acids in the spermatozoa of the participants.</li>
  <li>fatty_acids_serum: The serum levels of the fatty acids of the phospholipids (measured from the blood of the participant).</li>
  <li>sex_hormones: The serum levels of sex hormones measured in the blood of the participants.</li>
  <li>study_participant_related_data: General information about the participants such as age, abstinence time, and Body Mass Index (BMI).</li>
  <li>videos: Overview of which video file belongs to what participant.</li>
</ul>

<p>In addition to the main dataset, VISEM-Tracking [1], we provide an additional graph dataset which was extracted from the VISEM-Tracking. More details about this graph dataset can be found here: https://huggingface.co/datasets/SimulaMet-HOST/visem-tracking-graphs.</p>

<ul>
  <li><a href="https://zenodo.org/record/7293726">Development Dataset</a></li>
  <li><a href="https://huggingface.co/datasets/SimulaMet-HOST/visem-tracking-graphs">Development Dataset - Graph Data</a></li>
  <li><a href="https://www.dropbox.com/sh/3g9vz525qxl172d/AABenhYYqvUDOOmiA_H67vLPa?dl=0">Testing Dataset</a></li>
</ul>

<h4 id="ground-truth">Ground truth</h4>
<p>The ground truth data provided in this task were prepared by expert computer scientists and verified by domain experts. Tracking ground truth uses the YOLO format while the motility ground truth is a CSV file containing the motility values.</p>

<h4 id="evaluation-methodology">Evaluation methodology</h4>
<p>For Task 1 and Task 2 will be evaluated using standard detection and tracking metrics. For detection, this includes precision, recall, mAP@50, and mAP@50-95. For tracking we use Jonathan Luiten’s TrackEval library, which includes HOTA and other MOT evaluation metrics. Efficiency will be evaluated based on the number of samples that can be processed per second. Task 1 will only focus on the prediction metrics, while Task 2 will be weighted by the speed of the system.</p>

<p>For Task 3 and Task 4, we can use Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) to evaluate the predictions.</p>

<p>The evaluation scripts are available here: https://github.com/LouisDo2108/MediaEval2023-Medico-EvalScript</p>

<h4 id="submission-instructions">Submission Instructions</h4>

<h5 id="task-1">Task 1:</h5>
<p>All files of Task 1, as detailed below, should be compressed into a single .zip file and uploaded to the designated section for Task 1 on the submission form.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source_code
   	|- code_and_checkpoints
|- README.txt (must explain how to run your model to detect sperms on a new video)
	|- run.sh (shell script file to run your models for new video inputs (.mp4))

predictions
|- &lt;test_video_ id&gt;
	|- labels
             |- &lt;video id&gt;_frame_0.txt
             |- &lt;video id&gt;_frame_1.txt
             |- &lt;video id&gt;_frame_2.txt
              ...
 
    |-labels_ftid (optional) # labels with unique feature IDs to track them via multiple frames
             |- &lt;video id&gt;_frame_0.txt with tracking IDs.
             |- &lt;video id&gt;_frame_1.txt with tracking IDs.
             |- &lt;video id&gt;_frame_2.txt with tracking IDs.
             ...
     	|- &lt;video id&gt;.mp4 (showing sperm detection information)
  	|- &lt;video id&gt;_tracking.mp4 (showing sperm tracking information) - optional
	|- &lt;video id&gt;_detection.json
	|- tracking (same structure as this folder “VISEM_Tracking_Train_v4\trackeval_MOT\trackers\mot_challenge\MOT17-test\&lt;any_name_you_want&gt;\data” in the provided example  file (https://drive.google.com/file/d/1nSsQbAMxCmZoLeEAQwVLYVbQA7zq2WQG/view?usp=sharing))

|- …
</code></pre></div></div>

<p>The ‘predictions’ folder should also contain the result JSON from the detection subtask and the optional tracking subtask as described in https://github.com/LouisDo2108/MediaEval2023-Medico-EvalScript#subtask-1-sperm-detection-and-tracking. The .txt files are also required in the case the JSONs are ill-formatted. This is an example format for the <video>_detection.json:</video></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
    {
        "bbox": [
            404.25,
            260.75,
            19.0,
            18.5
        ],
        "category_id": 0,
        "image_id": 1,
        "score": 0.84277
    },
    ...
]
</code></pre></div></div>
<p>Make sure the “image_id” matches the annotations/Train.json or annotations/Val.json in the provided files. We provide an example detection_example_prediction.json in the example data.</p>

<h5 id="task-2">Task 2</h5>
<p>All files for Task 2, as described below, should be compressed into a single .zip file and uploaded to the designated section for Task 2 on the submission form. We will execute your final model using our hardware resources; therefore, please ensure that your run.sh script functions correctly.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source_code
   	|- code_and_checkpoints
|- README.txt (must explain how to run your model to detect sperms on a new video)
	|- run.sh (shell script file to run your models for new video inputs (.mp4))

predictions
|- &lt;test_video_ id&gt;
	|- labels
             |- &lt;video id&gt;_frame_0.txt
             |- &lt;video id&gt;_frame_1.txt
             |- &lt;video id&gt;_frame_2.txt
              ...
 
    |-labels_ftid (optional) # labels with unique feature IDs to track them via multiple frames
             |- &lt;video id&gt;_frame_0.txt with tracking IDs.
             |- &lt;video id&gt;_frame_1.txt with tracking IDs.
             |- &lt;video id&gt;_frame_2.txt with tracking IDs.
             ...
     	|- &lt;video id&gt;.mp4 (showing sperm detection information)
  	|- &lt;video id&gt;_tracking.mp4 (showing sperm tracking information) - optional
|- ...
</code></pre></div></div>

<h5 id="task-3">Task 3</h5>
<p>We will compare your results with a ground truth file similar to ‘semen_analysis_data_Train.csv.’ Therefore, you are required to predict the following: Progressive Motility (%), Non-Progressive Sperm Motility (%), and Immotile Sperm (%). Please refer to the CSV file to locate these specific columns. Note that the sum of these three values should equal 100%. All files for Task 3, as described below, should be compressed into a single .zip file and uploaded to the designated section for Task 3 on the submission form.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>– source_code
	|– code_and_checkpoints
	|– README.txt (must explain how to run your model to predict motility level of a new video)
	|– run.sh (shell script file to run your models for new video inputs (.mp4)) # must work with test video files

– motility_predictions.csv

--------------
Sample format
--------------
ID, Progressive motility (%), Non progressive sperm motility (%), Immotile sperm (%)
1, 25, 75, 25
2, 45, 35, 20
…
</code></pre></div></div>
<p>The motility_predictions.csv should have the exact columns as the ground-truth and contains only the ID of the test set.</p>

<h5 id="task-4">Task 4</h5>
<p>This task is experimental in nature. You are required to utilize your prediction models from either Task 1, Task 2, or both. Graph structures can be prepared using the predicted bounding boxes. Sample source code for generating these graphs can be accessed here: https://github.com/vlbthambawita/visem-tracking-graphs . An example video graph structure is available here: https://huggingface.co/datasets/SimulaMet-HOST/visem-tracking-graphs/tree/main/spatial_threshold_0.1/11 . Additional details about the graphs using development data can be found here: https://huggingface.co/datasets/SimulaMet-HOST/visem-tracking-graphs . All files related to Task 4, as detailed below, should be compressed into a single .zip file and uploaded to the designated section for Task 4 on the submission form.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>– source_code
	|– code_and_checkpoints
	|- graph_data_stuctures
	|– README.txt (must explain how to run your model to predict motility level of a new video using graph stuctures)
	|– run.sh (shell script file to run your models for new video graph structures) 
– motility_predictions.csv

--------------
Sample format
--------------
ID, Progressive motility (%), Non progressive sperm motility (%), Immotile sperm (%)
1, 25, 75, 25
2, 45, 35, 20
…
</code></pre></div></div>

<p>Google form for submission: <a href="https://forms.gle/5EYy2zVrjhbh9ZzU8">https://forms.gle/5EYy2zVrjhbh9ZzU8</a></p>

<h4 id="quest-for-insight">Quest for insight</h4>
<ul>
  <li>How accurate are deep learning methods for identifying sperms in a fresh sample?</li>
  <li>Will continued tracking of sperm help to analyze the motility level of sperm samples?</li>
  <li>How do we calculate the average speed of moving sperms, and how to track the fastest one among many moving sperms?</li>
  <li>How can we convince doctors about the accuracy, reliability, and trustworthiness of the output of Deep Learning methods?</li>
  <li>Comprehensive description of pre- and post-processing methods.</li>
  <li>Explain why specific processing strategies were chosen and what insights were used to consider them.</li>
</ul>

<h4 id="participant-information">Participant information</h4>
<p>Please contact your task organizers with any questions on these points.</p>
<ul>
  <li>Signing up: Fill in the <a href="https://forms.gle/FFS8V3zbijXoqEWJ6">registration form</a> and fill out and return the <a href="https://multimediaeval.github.io/editions/2023/docs/MediaEval2023_UsageAgreement.pdf">usage agreement</a>.</li>
  <li>Making your submission: See the submission requirements in Medicos official GitHub repository.</li>
  <li>Preparing your working notes paper: Instructions on preparing you working notes paper can be found in <a href="https://docs.google.com/document/d/1HcAx14RVuxqDEi-1SJJRwhHhzC_V-Ktpw-9jn5dg0-0">MediaEval 2023 Working Notes Paper Instructions</a>.</li>
</ul>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>
<p>[1] Thambawita, V., Hicks, S.A., Storås, A.M. et al. VISEM-Tracking, a human spermatozoa tracking dataset. Sci Data 10, 260 (2023). https://doi.org/10.1038/s41597-023-02173-4</p>

<p>[2] Trine B. Haugen, Steven A. Hicks, Jorunn M. Andersen, Oliwia Witczak, Hugo L. Hammer, Rune Borgli, Pål Halvorsen, and Michael Riegler. 2019. VISEM: a multimodal video dataset of human spermatozoa. The Proceedings of the 10th ACM Multimedia Systems Conference (MMSys ‘19). Association for Computing Machinery, New York, NY, USA, 261–266. DOI:https://doi.org/10.1145/3304109.3325814</p>

<p>[3] Hicks, S.A., Andersen, J.M., Witczak, O. et al. Machine Learning-Based Analysis of Sperm Videos and Participant Data for Male Fertility Prediction. Sci Rep 9, 16770 (2019). https://doi.org/10.1038/s41598-019-53217-y</p>

<p>[4] Thambawita, V., Halvorsen, P., Hammer, H., Riegler, M., &amp; Haugen, T. B. (2019). Stacked dense optical flows and dropout layers to predict sperm motility and morphology. arXiv preprint arXiv:1911.03086.</p>

<p>[5] Thambawita, V., Halvorsen, P., Hammer, H., Riegler, M., &amp; Haugen, T. B. (2019). Extracting temporal features into a spatial domain using autoencoders for sperm video analysis. arXiv preprint arXiv:1911.03100.</p>

<h4 id="task-organizers">Task organizers</h4>
<p><strong>Organizers</strong></p>
<ul>
  <li>Vajira Thambawita, vajira (at) simula.no, SimuaMet</li>
  <li>Steven Hicks, steven (at) simula.no, SimulaMet</li>
  <li>Andrea Storås andrea (at) simula.no, SimulaMet</li>
  <li>Michael Riegler, michael (at) simula.no, SimulaMet</li>
  <li>Pål Halvorsen, paalh (at) simula.no, SimulaMet</li>
</ul>

<p><strong>Co-organizers</strong></p>
<ul>
  <li>Tuan-Luc Huynh, htluc (at) selab.hcmus.edu.vn, University of Science - VNUHCM</li>
  <li>Hai-Dang Nguyen, nhdang (at) selab.hcmus.edu.vn, University of Science - VNUHCM</li>
  <li>Minh-Triet Tran, tmtriet (at) selab.hcmus.edu.vn, University of Science - VNUHCM</li>
  <li>Trung-Nghia Le, ltnghia (at) selab.hcmus.edu.vn, University of Science - VNUHCM</li>
</ul>

<h4 id="task-schedule">Task schedule</h4>
<ul>
  <li>31st August 2023: Development data release <!-- * XX XXX 2023: Data release <!-- # Replace XX with your date. We suggest setting the date in July-October. 13 October is absolute very last possible date by which you should release data. You can release earlier, or plan a two-stage release.--></li>
  <li>15th October 2023: Test data release</li>
  <li>15th November 2023: Runs due. Exact dates to be announced. <!--* XX November 2023: Runs due <!-- # Replace XX with your date. We suggest setting enough time in order to have enough time to assess and return the results by the Results returned.--></li>
  <li>8 December 2023: Results returned  <!-- Replace XX with your date. Latest possible date is 8 December 2023--></li>
  <li>15 December 2023: Working notes paper  <!-- Fixed. Please do not change.--></li>
  <li>1-2 February 2024: 14th Annual MediaEval Workshop, Collocated with <a href="https://mmm2024.org/">MMM 2024</a> in Amsterdam, Netherlands and also online. <!-- Fixed. Please do not change.--></li>
</ul>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
