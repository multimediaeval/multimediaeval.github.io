<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MediaEval 2026 | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="MediaEval 2026" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data, with a focus on analysis, exploration, and information access and retrieval. Signup is open to anyone who wishes to participate. There are two types of participation: “standard” participation requires creating an algorithm for addressing the challenge and “insight” participation requires gaining new insight, e.g., into the data, the evaluation procedure, or the implications of the task. Participating teams submit their algorithms (for the “standard” participation) and write up papers for the working notes proceedings (both “standard” and “insight” participation). The larger aim of MediaEval is to promote reproducible research that makes multimedia a positive force for society." />
<meta property="og:description" content="The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data, with a focus on analysis, exploration, and information access and retrieval. Signup is open to anyone who wishes to participate. There are two types of participation: “standard” participation requires creating an algorithm for addressing the challenge and “insight” participation requires gaining new insight, e.g., into the data, the evaluation procedure, or the implications of the task. Participating teams submit their algorithms (for the “standard” participation) and write up papers for the working notes proceedings (both “standard” and “insight” participation). The larger aim of MediaEval is to promote reproducible research that makes multimedia a positive force for society." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2026/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2026/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-26T13:43:02+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="MediaEval 2026" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-01-26T13:43:02+00:00","datePublished":"2026-01-26T13:43:02+00:00","description":"The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data, with a focus on analysis, exploration, and information access and retrieval. Signup is open to anyone who wishes to participate. There are two types of participation: “standard” participation requires creating an algorithm for addressing the challenge and “insight” participation requires gaining new insight, e.g., into the data, the evaluation procedure, or the implications of the task. Participating teams submit their algorithms (for the “standard” participation) and write up papers for the working notes proceedings (both “standard” and “insight” participation). The larger aim of MediaEval is to promote reproducible research that makes multimedia a positive force for society.","headline":"MediaEval 2026","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2026/"},"url":"https://multimediaeval.github.io/editions/2026/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>MediaEval 2026</h2>
  </header>

  <p>The MediaEval Multimedia Evaluation benchmark offers challenges in artificial intelligence for multimedia data, with a focus on analysis, exploration, and information access and retrieval. Signup is open to anyone who wishes to participate. There are two types of participation: “standard” participation requires creating an algorithm for addressing the challenge and “insight” participation requires gaining new insight, e.g., into the data, the evaluation procedure, or the implications of the task. Participating teams submit their algorithms (for the “standard” participation) and write up papers for the working notes proceedings (both “standard” and “insight” participation). The larger aim of MediaEval is to promote reproducible research that makes multimedia a positive force for society.</p>

<p>In the 2026 edition, we will offer the same slate of tasks as in the 2025 edition in order to provide opportunities for teams who were not able to participate in 2025. Also, we will put special emphasis on “insight” participation. Specifically, we welcome “Quest for Insight” papers that examine characteristics of the data and the task definitions, the strengths and weaknesses of particular types of approaches, observations about the evaluation procedure, and implications of the task.</p>

<p>Participating teams present their work at the annual MediaEval workshop. The workshop will take place in Amsterdam, Netherlands, and also offer the possibility of online participation. If you are an early career researcher and funding restrictions are preventing your participation, please inquire about possible travel support. This edition will be co-located with ACM International Conference on Multimedia Retrieval - <a href="https://icmr2026.org/">ICMR2026</a>.</p>

<h5 id="registration">Registration:</h5>
<p>Signup for MediaEval 2026 opens in January.</p>

<h5 id="mediaeval-2026-schedule">MediaEval 2026 Schedule:</h5>
<ul>
  <li>Registration for task participation opens: January 2026</li>
  <li>Test data release: 1 March 2026</li>
  <li>Runs due: 1 May 2026</li>
  <li>Working notes papers due: 31 May 2026</li>
  <li>MediaEval 2026 Workshop, Sat.-Sun. 15-16 June 2026, Amsterdam, Netherlands and Online, co-located with ACM ICMR 2026</li>
</ul>

<h5 id="the-mediaeval-coordination-committee-2026">The MediaEval Coordination Committee (2026):</h5>
<ul>
  <li>Mihai Gabriel Constantin, National University of Science and Technology Politehnica Bucharest, Romania</li>
  <li>Steven Hicks, SimulaMet, Norway</li>
  <li>Martha Larson, Radboud University, Netherlands</li>
</ul>


  
  

  
  <h3>
    Task List
  </h3>

  
  <ul class="right hide-on-med-and-down"></ul>
  
    <a href="https://multimediaeval.github.io/editions/2026/tasks/medico/"><h4>Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging</h4></a>
    <p>Medico 2026 focuses on Visual Question Answering (VQA) for gastrointestinal (GI) imaging, with an emphasis on explainability, clinical safety, and multimodal reasoning. The task leverages the expanded Kvasir-VQA-x1 dataset, containing more than 150,000 clinically relevant question–answer pairs, to support the development of AI models that can accurately answer questions based on GI endoscopy images while providing coherent and clinically grounded explanations. The goal is to advance trustworthy and interpretable AI decision support for GI diagnostics.</p>
    <a href="https://multimediaeval.github.io/editions/2026/tasks/medico/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2026/tasks/memorability/"><h4>Memorability: Predicting movie and commercial memorability</h4></a>
    <p>The goal of this task is to study the long-term memory performance when recognising small movie excerpts or commercial videos. We provide the videos, precomputed features or EEG features for the challenges proposed in the task such as how memorable a video, if a person is familiar with a video or if you can predict the brand memorability?</p>
    <a href="https://multimediaeval.github.io/editions/2026/tasks/memorability/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2026/tasks/newsimages/"><h4>NewsImages: Retrieval and generative AI for news thumbnails</h4></a>
    <p>Participants receive a large set of English-language articles from international publishers. We provide a comprehensive dataset of news articles, including article title, article lead, URL (for retrieving the full text), and an editorially assigned image. Given the text of a news article, the task is to provide a fitting image. Teams are encouraged to explore different image retrieval and image generation techniques to provide article-specific visuals.</p>
    <a href="https://multimediaeval.github.io/editions/2026/tasks/newsimages/">Read more.</a>
    <br>
  
    <a href="https://multimediaeval.github.io/editions/2026/tasks/synthim/"><h4>Synthetic Images: Advancing detection and localization of generative AI used in real-world online images</h4></a>
    <p>The goal of this challenge is to develop AI models capable of detecting synthetic images and identifying the specific regions in the images that have been manipulated or synthesized.</p>
    <a href="https://multimediaeval.github.io/editions/2026/tasks/synthim/">Read more.</a>
    <br>
  
</ul>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
