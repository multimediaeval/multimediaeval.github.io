<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Missing Pieces and Misinformation: Identifying social media posts with implicit messages (pilot task) | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Missing Pieces and Misinformation: Identifying social media posts with implicit messages (pilot task)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2026 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2026 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2026/tasks/enthymeme/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2026/tasks/enthymeme/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-16T13:30:11+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Missing Pieces and Misinformation: Identifying social media posts with implicit messages (pilot task)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-02-16T13:30:11+00:00","datePublished":"2026-02-16T13:30:11+00:00","description":"See the MediaEval 2026 webpage for information on how to register and participate.","headline":"Missing Pieces and Misinformation: Identifying social media posts with implicit messages (pilot task)","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2026/tasks/enthymeme/"},"url":"https://multimediaeval.github.io/editions/2026/tasks/enthymeme/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Missing Pieces and Misinformation: Identifying social media posts with implicit messages (pilot task)</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2026/">MediaEval 2026 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task Description</h4>

<p>The goal of this task is to develop AI models that are capable of detecting implicit arguments
(enthymemes) in tweets. The dataset contains tweets and their annotations, and also includes
reconstructions of the full arguments: propositions, premises, conclusions, and argument
structures (schemes).</p>

<p>Participants are invited to complete two tasks. While they may choose to complete only task 1, completion of task 2 is conditional upon prior completion of task 1.</p>

<p><strong>Task 1: “Enthymeme Detection”</strong> — Detecting the absence or presence of enthymemes in tweets
(binary classification)</p>
<ul>
  <li><em>Constrained Run 1:</em> From the text only, predict the presence or absence of implicit arguments.</li>
  <li><em>Constrained Run 2:</em> Annotation labels from 3 different annotators should also be leveraged
and investigated to increase performance.</li>
  <li><em>Open Run:</em> Using any external data, predict the presence or absence of implicit arguments.</li>
</ul>

<p><strong>Task 2: “Proposition Generation”</strong> — Generate the missing piece, i.e., the full implicit
proposition. In the example below, the AI system would not only detect the presence or absence
of an implicit statement, but in case of detecting its presence, it would generate the text of
the missing/implicit proposition (i.e., the text of Premise 1 below).</p>

<p><strong>Example:</strong></p>

<p>If the tweet contains the following text: <em>“Deterring the plans of illegal people smugglers is essential to controlled immigration. We should support all plans to stop them.”</em></p>

<p>This tweet was annotated with the presence of an implicit premise: <em>” Controlled immigration is desirable
“</em></p>

<p>The full argument can be reconstructed as:</p>
<ul>
  <li>Premise 1 <em>(implicit)</em>:  Controlled immigration is desirable</li>
  <li>Premise 2: Deterring the plans of illegal people smugglers is essential to controlled immigration</li>
  <li>Conclusion: We should support all plans to stop them.</li>
</ul>

<p>Participating teams will write short working-notes papers that are published in the workshop
proceedings (optional). We welcome two types of papers: first, conventional benchmarking papers,
which describe the methods that teams use to address the task (enthymeme detection and implicit
proposition generation) and analyze the results across the constrained and open runs; and second,
“Quest for Insight” papers, which address a research question aimed at gaining deeper
understanding of implicit argumentation, but do not necessarily present complete task results.
Example questions for “Quest for Insight” papers include: How do different annotators interpret
implicit premises? What linguistic features best signal the presence of enthymemes?</p>

<h4 id="motivation-and-background">Motivation and Background</h4>

<p>Enthymemes—arguments with missing components (premises or conclusions)—represent a fundamental
challenge in understanding persuasive discourse and argumentation. These implicit arguments are
particularly prevalent in social media contexts, where they serve as powerful means of persuasion
(Lombardi Vallauri et al., 2020). By leaving key premises/conclusions unstated, enthymemes lead
readers to perceive the implicit content as their own reasoning (Reboul, 2011), making them
especially effective rhetorical devices.</p>

<p>The significance of detecting and reconstructing enthymemes extends beyond theoretical interest
in argumentation theory. Enthymemes facilitate deceptive argumentation and manipulation, and help
in spreading disinformation (Lombardi Vallauri et al., 2020). Understanding how implicit premises
operate in controversial political discourse is therefore crucial for developing tools to combat
misinformation and promote critical thinking.</p>

<p>The task of enthymeme detection can be framed as a binary classification problem: determining
whether a given text contains an implicit argument or not. This simple formulation is interesting
for several reasons. First, it provides a foundational step for more complex argument mining
pipelines—before attempting to reconstruct missing propositions, systems must first identify where
implicit argumentation occurs. Second, binary classification allows for systematic investigation
of what linguistic and discourse characteristics signal the presence of enthymemes, enabling both
interpretable models and empirical validation of theoretical claims about argumentation structure.</p>

<p>However, developing computational systems for enthymeme detection and reconstruction presents
considerable challenges. The task is inherently interpretative, involving natural language
inference and semantic interpretation where high human disagreement is common (Plank et al., 2014;
Aroyo &amp; Welty, 2015). Language tasks of this nature involve interpretation, multiple plausible
answers, and indirect meanings (Pavlick &amp; Kwiatkowski, 2019), and relying on a single “correct”
label ignores rich variation in human judgments (Uma et al., 2021).</p>

<p>Our approach explicitly acknowledges the interpretative nature of the task by employing three
independent annotators per instance, a design choice that would enable us to treat human label
variation as signal rather than noise. This resource adds on an existing dataset of tweets
(Flaccavento et al., 2025) and provides the first annotated dataset specifically designed for
investigating enthymemes in controversial political discourse, enabling research into how discourse
characteristics of enthymemes can improve their detection with NLP methods.</p>

<h4 id="target-group">Target Group</h4>

<p>This task is interesting to anyone who is interested in text analysis. We expect it to attract
people working in areas such as natural language processing, argument mining, computational
linguistics, misinformation detection, and social media analysis, from both academic and
industrial settings.</p>

<p>We especially welcome interdisciplinary teams, including researchers from argumentation theory,
philosophy, rhetoric, communication studies, political science, and computational social science,
as these perspectives are essential for understanding how implicit argumentation influences
persuasion, shapes political discourse, and affects the processes by which audiences interpret and
reason about controversial topics. The use of explicit structural modeling, linguistic
feature-based approaches, and even rule-based systems of all sorts are encouraged.</p>

<h4 id="data">Data</h4>

<p>The dataset consists of tweets that have been annotated by multiple annotators who judged whether
or not the tweet contains an enthymeme. For each enthymeme, the annotators also propose a
reconstruction of the implicit and explicit propositional content and argument structure in cases
of enthymeme presence. The tweets are a subset of the tropes dataset by Flaccavento et al.
(2025), which was selected to include a balance of tweets on the topics of immigration in the UK
and the COVID-19 vaccine.</p>

<p>The data will be released in three parts:</p>

<ul>
  <li><strong>Data sample (1 March):</strong> A small collection of tweets that have been annotated by two
annotators, so that participants can read the data and understand the challenge of the task.</li>
  <li><strong>First data release (Mid-March):</strong> A larger collection of data that has been annotated by five
annotators. It is split into train and dev.</li>
  <li><strong>Final dataset (Beginning April):</strong> The full dataset, which is split into train, dev, and test
set. The train set is a superset of the training set released with the first data release.
Likewise, the dev set is a superset of the development set released with the first data release.
Train and dev data has been annotated by five annotators. The test set is the test set for the
task: the participants are required to submit their predictions on the test set.</li>
</ul>

<blockquote>
  <p>⚠️ Participants should be aware that the data contains language hurtful towards immigrants and
should be ready for this when reading the data.</p>
</blockquote>

<h4 id="evaluation-methodology">Evaluation Methodology</h4>

<p><strong>Task 1:</strong> Since this is a label prediction task, we will evaluate using F1 concerning the
presence or absence of enthymemes. Three labels are considered in the basic setting:
<code class="language-plaintext highlighter-rouge">implicit_premise</code>, <code class="language-plaintext highlighter-rouge">implicit_conclusion</code>, and <code class="language-plaintext highlighter-rouge">none</code>.</p>

<p><strong>Task 2:</strong> The generated propositions will be evaluated in two ways. First, BERTScore will be
used to compare the reconstructions provided by the annotators with the propositions generated by
the participants. Second, a subset of the test set will be sampled and evaluated by hand by
experienced human annotators.</p>

<h4 id="quest-for-insight">Quest for Insight</h4>

<ul>
  <li>What systematic patterns emerge in label variation across easy-medium-hard cases, and do they
reveal distinct interpretative frameworks?</li>
  <li>Does modeling the full distribution of human judgments improve performance on borderline cases
compared to majority-vote labels?</li>
  <li>Can annotator disagreement patterns predict which instances will be hardest for NLP models to
classify?</li>
  <li>What sorts of linguistic or discourse features can be leveraged to improve classification
performance?</li>
  <li>What additional data can be used or is relevant to improve classification performance?</li>
  <li>How does average inter-annotator agreement compare to other evaluation schemes in assessing
annotation performance when no single ground truth exists?</li>
  <li>What is the most effective way to leverage annotator reconstructions to evaluate implicit
proposition generation performance?</li>
</ul>

<h4 id="task-organizers">Task Organizers</h4>

<ul>
  <li><strong>Martial Pastor</strong>, Radboud University — martial.pastor@ru.nl</li>
  <li><strong>Nelleke Oostdijk</strong>, Radboud University — nelleke.oostdijk@ru.nl</li>
</ul>

<p><em>Data will be made available as of the 1st of March.</em></p>

<h4 id="references">References</h4>

<p>[1] Aroyo, L., &amp; Welty, C. (2015). Truth is a lie: Crowd truth and the seven myths of human annotation. <em>AI Magazine, 36</em>(1), 15–24.</p>

<p>[2] Flaccavento, A., Peskine, Y., Papotti, P., Torlone, R., &amp; Troncy, R. (2025, January). Automated detection of tropes in short texts. In <em>Proceedings of COLING 2025: 31st International Conference on Computational Linguistics.</em></p>

<p>[3] Plank, B., Hovy, D., &amp; Søgaard, A. (2014). Linguistically debatable or just plain wrong? In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</em> (Vol. 2: Short Papers, pp. 507–511). Association for Computational Linguistics.</p>

<p>[4] Reboul, A. (2011). A relevance-theoretic account of the evolution of implicit communication. <em>Studies in Pragmatics, 13</em>(1).</p>

<p>[5] Uma, A., Fornaciari, T., Hovy, D., Paun, S., Plank, B., &amp; Poesio, M. (2021). Learning from disagreement: A survey. <em>Journal of Artificial Intelligence Research, 72,</em> 1385–1470.</p>

<p>[6] Vallauri, E. L., Baranzini, L., Cimmino, D., Cominetti, F., Coppola, C., &amp; Mannaioli, G. (2020). Implicit argumentation and persuasion: A measuring model. <em>Journal of Argumentation in Context, 9,</em> 95–123.</p>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
