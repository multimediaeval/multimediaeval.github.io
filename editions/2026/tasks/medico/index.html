<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2026 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2026 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2026/tasks/medico/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2026/tasks/medico/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-26T13:55:34+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-01-26T13:55:34+00:00","datePublished":"2026-01-26T13:55:34+00:00","description":"See the MediaEval 2026 webpage for information on how to register and participate.","headline":"Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2026/tasks/medico/"},"url":"https://multimediaeval.github.io/editions/2026/tasks/medico/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2026/" style="color:white;">MediaEval 2026</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2025/">MediaEval 2025</a></li>
  
  <li><a href="/editions/2023/">MediaEval 2023</a></li>
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Medico 2026: Visual Question Answering (VQA) for Gastrointestinal Imaging</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2026/">MediaEval 2026 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task description</h4>

<p>Gastrointestinal (GI) diseases represent a major global health burden, where accurate interpretation of endoscopy findings is critical for diagnosis and treatment planning. While AI systems have demonstrated strong 
performance in GI image analysis, their clinical adoption remains limited by insufficient explainability, safety concerns, and a lack of alignment with clinical reasoning.</p>

<p>Building on the success of previous Medico challenges and closely aligned with the ImageCLEFmedical-MEDVQA 2026 initiative, Medico 2026 advances medical VQA for GI imaging with a continued emphasis on interpretability and 
reliability. Medical VQA combines computer vision and natural language processing to answer clinically meaningful questions derived from medical images. However, existing approaches often prioritize answer accuracy without 
sufficiently addressing explanation quality, safety, or clinical consistency.</p>

<p>The Medico 2026 challenge therefore emphasizes not only correct answers but also multimodal explanations that combine textual and visual evidence and adhere to medical best practices. In addition, the task introduces 
evaluation criteria targeting behavioral safety, discouraging undesirable model behaviors such as overconfident answers, misleading justifications, or clinically inappropriate reasoning.</p>

<p>Participating teams will write short working-notes papers that are published in the MediaEval Workshop Working Notes Proceedings. We welcome two types of papers: first, conventional benchmarking papers, which describe 
the methods that the teams use to address the task and analyze the results and, second, “Quest for Insight” papers, which address a question aimed at gaining more insight into the task, but do not necessarily present 
task results. Example questions for “Question for Insight” papers are below.</p>

<p><strong>Subtask 1: Medical Image Question Answering in GI Endoscopy</strong></p>

<p>This subtask focuses on developing models that accurately answer clinically relevant questions based on GI endoscopy images using the Kvasir-VQA-x1 dataset, which contains more than 150,000 question–answer pairs. 
The dataset is derived from established GI endoscopy collections and covers a wide range of anatomical regions, pathological findings, and medical instruments.</p>

<p>Questions span multiple categories, including Yes/No, Single-Choice, Multiple-Choice, Color-Related, Location-Related, and Numerical Count, requiring joint reasoning over visual and textual information. Model 
performance is evaluated using quantitative metrics assessing answer correctness and language accuracy.</p>

<p><strong>Subtask 2: Explainable and Safe Multimodal Reasoning for GI VQA</strong></p>

<p>This subtask extends Subtask 1 by requiring models to provide coherent multimodal explanations that justify their answers. Explanations must combine textual reasoning with visual evidence, such as highlighted 
image regions, in a manner aligned with clinical reasoning.</p>

<p>In addition to interpretability, this subtask introduces a dedicated safety layer that evaluates model behavior across clinical contexts. Models are assessed for undesirable behaviors, including overconfidence, 
misleading explanations, or non-compliance with established medical best practices. To support retrieval-augmented reasoning, participants may leverage a curated database of verified endoscopy resources provided as 
part of the challenge.</p>

<h4 id="motivation-and-background">Motivation and background</h4>

<p>For AI systems to be integrated into clinical workflows, they must be transparent, interpretable, and safe. In GI imaging, deep learning models have achieved promising results for classification and detection tasks, 
yet their black-box nature limits trust among clinicians. Medical professionals require explanations that clearly connect visual evidence to clinical conclusions.</p>

<p>Medical VQA offers a natural interface for explainable decision support, enabling clinicians to ask structured questions and receive interpretable responses. Nevertheless, many existing VQA models provide answers without 
sufficient justification or safeguards against unsafe reasoning. Medico 2026 addresses these limitations by explicitly integrating explainability and safety into both task design and evaluation. By encouraging multimodal 
explanations and clinically consistent behavior, the challenge aims to advance AI systems that support, rather than replace, clinical expertise.</p>

<h4 id="target-group">Target group</h4>

<p>The task targets researchers from multimedia analysis, computer vision, natural language processing, and medical AI communities. Consistent with previous Medico challenges, the task is designed to be accessible 
to both experienced researchers and newcomers to medical AI. Mentoring, baseline implementations, and starter documentation will be provided to support undergraduate and graduate students.</p>

<h4 id="data">Data</h4>

<p>Medico 2026 uses the Kvasir-VQA-x1 dataset, an expanded GI endoscopy VQA dataset containing more than 150,000 annotated question–answer pairs. The dataset builds on established GI endoscopy image collections and 
is curated with clinical input to ensure medical relevance and correctness. Questions are designed to assess visual understanding, clinical interpretation, and reasoning across a diverse set of GI conditions and 
procedures.</p>

<h4 id="evaluation-methodology">Evaluation methodology</h4>

<p>Subtask 1 Evaluation. Models are evaluated using quantitative metrics for answer correctness and language accuracy, including accuracy, precision, recall, and F1 score.</p>

<p>Subtask 2 Evaluation. In addition to Subtask 1 metrics, Subtask 2 includes expert-based evaluation of explanation quality. Explanations are assessed for clarity, coherence, medical relevance, and consistency with 
visual evidence. Safety-oriented criteria evaluate whether model outputs demonstrate appropriate uncertainty, factual correctness, and adherence to clinical best practices.</p>

<h4 id="quest-for-insight">Quest for insight</h4>

<ul>
  <li>How can VQA models generate explanations that align with clinical reasoning in GI diagnostics?</li>
  <li>Which multimodal techniques best support transparent and safe medical VQA?</li>
  <li>How can retrieval-augmented reasoning improve factual consistency and clinical reliability?</li>
  <li>What evaluation strategies best capture explanation quality and behavioral safety in medical AI?</li>
  <li>How can models balance accuracy, interpretability, and safety in GI VQA tasks?</li>
</ul>

<h4 id="risk-management">Risk management</h4>

<p>The Medico task series has been successfully organized for multiple years. For the 2026 edition, baseline models, starter code, and detailed documentation will be provided. Previous participants will be 
actively invited, and continuous support will be offered throughout the challenge to mitigate technical and organizational risks.</p>

<h4 id="task-organizers">Task organizers</h4>

<ul>
  <li>Sushant Gautam, SimulaMet, Norway</li>
  <li>Vajira Thambawita, SimulaMet, Norway</li>
  <li>Pål Halvorsen, SimulaMet, Norway</li>
  <li>Michael A. Riegler, SimulaMet, Norway</li>
  <li>Steven A. Hicks, SimulaMet, Norway</li>
</ul>



      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
