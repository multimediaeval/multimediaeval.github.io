<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="See the MediaEval 2021 webpage for information on how to register and participate." />
<meta property="og:description" content="See the MediaEval 2021 webpage for information on how to register and participate." />
<link rel="canonical" href="https://multimediaeval.github.io/editions/2021/tasks/sportsvideo/" />
<meta property="og:url" content="https://multimediaeval.github.io/editions/2021/tasks/sportsvideo/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-06-14T14:14:55+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-06-14T14:14:55+00:00","datePublished":"2023-06-14T14:14:55+00:00","description":"See the MediaEval 2021 webpage for information on how to register and participate.","headline":"Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://multimediaeval.github.io/editions/2021/tasks/sportsvideo/"},"url":"https://multimediaeval.github.io/editions/2021/tasks/sportsvideo/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2023/" style="color:white;">MediaEval 2023</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2023/" style="color:white;">MediaEval 2023</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2022/">MediaEval 2022</a></li>
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
          <header id="main" style="background-image: url('')">
    <h2>Sports Video: Fine Grained Action Detection and Classification of Table Tennis Strokes from videos</h2>
  </header>

  <!-- # please respect the structure below-->
<p><em>See the <a href="https://multimediaeval.github.io/editions/2021/">MediaEval 2021 webpage</a> for information on how to register and participate.</em></p>

<h4 id="task-description">Task Description</h4>
<p>This task offers researchers an opportunity to test their fine-grained classification methods for detecting and recognizing strokes in table tennis videos. (The low inter-class variability makes the task more difficult than with usual general datasets like UCF-101.) The task offers two subtasks:</p>

<p><strong><em>Subtask 1: Stroke Detection:</em></strong> Participants are required to build a system that detects whether a stroke has been performed, whatever its class, and to extract its temporal boundaries. The aim is to be able to distinguish between moments of interest in a game (players performing strokes) from irrelevant moments (between strokes, picking up the ball, having a break…). This subtask can be a preliminary step for later recognizing a stroke that has been performed.</p>

<p><strong><em>Subtask 2: Stroke Classification:</em></strong> Participants are required to build a classification system that automatically labels video segments according to a performed stroke. There are 20 possible stroke classes.</p>

<p>Compared with <a href="https://multimediaeval.github.io/editions/2020/tasks/sportsvideo/">Sports Video 2020</a>, this year we extend the task in the direction of detection and also enrich the dataset with new and more diverse stroke samples. The overview paper of the task is already available <a href="https://www.labri.fr/projet/AIV/MediaEval/Sports_Video_Task_2021.pdf">here</a>.</p>

<p>Participants are encouraged to make their code public with their submission. We provide a public baseline, have a look <a href="https://github.com/ccp-eva/SportTaskME21">here</a>.</p>

<h4 id="motivation-and-background">Motivation and background</h4>
<p>Action detection and classification are one of the main challenges in visual content analysis and mining. Sports video analysis has been a very popular research topic, due to the variety of application areas, ranging from analysis of athletes’ performances and rehabilitation to multimedia intelligent devices with user-tailored digests. Datasets focused on sports activities or datasets including a large amount of sports activity classes are now available and many research contributions benchmark on those datasets. A large amount of work is also devoted to fine-grained classification through the analysis of sports gestures using motion capture systems. However, body-worn sensors and markers could disturb the natural behavior of sports players. Furthermore, motion capture devices are not always available for potential users, be it a University Faculty or a local sports team. Giving end-users the possibility to monitor their physical activities in ecological conditions through simple equipment is a challenging issue. The ultimate goal of this research is to produce automatic annotation tools for sports faculties, local clubs and associations to help coaches better assess and advise athletes during training.</p>

<h4 id="target-group">Target group</h4>
<p>The task is of interest to researchers in the areas of machine learning, visual content analysis, computer vision and sports performance. We explicitly encourage researchers focusing specifically in domains of computer-aided analysis of sports performance.</p>

<h4 id="data">Data</h4>
<p>Our focus is on recordings that have been made by widespread and cheap video cameras, e.g., GoPro. We use a dataset specifically recorded at a sports faculty facility and continuously completed by students and teachers. This dataset is constituted of player-centered videos recorded in natural conditions without markers or sensors. It comprises 20 table tennis strokes, and a rejection class. The problem is hence a typical research topic in the field of video indexing: for a given recording, we need to label the video by recognizing each stroke appearing in it. The dataset is subject to a specific usage agreement accesible <a href="https://www.labri.fr/projet/AIV/MediaEval/Particular_conditions.pdf">here</a>.</p>

<h4 id="evaluation-methodology">Evaluation methodology</h4>
<p>Twenty stroke classes are considered according to the rules of table tennis. This taxonomy was designed with professional table tennis teachers. We are working on videos recorded at the Faculty of Sports of the University of Bordeaux. Students are the sportsmen filmed and the teachers are supervising exercises conducted during the recording sessions. The dataset has been recorded in a sports faculty facility using a light-weight equipment, such as GoPro cameras. The recordings are markerless and allow the players to perform in natural conditions from different viewpoints. These sequences were manually annotated, and the annotation sessions were supervised by professional players and teachers using a crowdsourced annotation platform.</p>

<p>The training dataset shared for each subtask is composed of videos of table tennis matches with temporal borders of performed strokes supplied in an xml file, with the corresponding stroke label.</p>

<p><strong><em>Subtask 1: Stroke Detection:</em></strong>  Participants are asked to temporally segment regions where a stroke is performed on unknown videos of matches. The mAP and IoU metrics on temporal segments will be used for evaluation.</p>

<p><strong><em>Subtask 2: Stroke Classification:</em></strong>  Participants produce an xml file where each stroke of test sequences is labeled according to the given taxonomy. Submissions will be evaluated in terms of accuracy per class and global accuracy.</p>

<p>For each subtask, participants may submit up to five runs. We also encourage participants to carry out a failure analysis of their results in order to gain insight into the mistakes that their classifiers make.</p>

<h4 id="leaderboard">Leaderboard</h4>

<h5 id="subtask-1-stroke-detection">Subtask 1: Stroke Detection</h5>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Rank</th>
      <th style="text-align: center">Team</th>
      <th style="text-align: center">mAP</th>
      <th style="text-align: center">Global IoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">0.0173</td>
      <td style="text-align: center">0.144</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">QuantEx</td>
      <td style="text-align: center">0.00124</td>
      <td style="text-align: center">0.070</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">SSNCSE</td>
      <td style="text-align: center">0.000525</td>
      <td style="text-align: center">0.247</td>
    </tr>
  </tbody>
</table>

<h5 id="subtask-2-stroke-classification">Subtask 2: Stroke Classification</h5>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Rank</th>
      <th style="text-align: center">Team</th>
      <th style="text-align: center">Global Acc in %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">INF</td>
      <td style="text-align: center">74.2</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">SELAB-HCMUS</td>
      <td style="text-align: center">68.8</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">20.4</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">SSNCSE</td>
      <td style="text-align: center">9.95</td>
    </tr>
  </tbody>
</table>

<h4 id="references-and-recommended-reading">References and recommended reading</h4>
<!-- # Please use the ACM format for references https://www.acm.org/publications/authors/reference-formatting (but no DOI needed)-->
<!-- # The paper title should be a hyperlink leading to the paper online-->
<!-- # The logic of the ordering of the papers is not clear. Can they be alphabetical?-->

<p>[1] <a href="https://github.com/P-eMartin/crisp">Crisp Project</a></p>

<p>[2] Pierre-Etienne Martin, Jenny Benois-Pineau, Renaud Péteri, Julien Morlier. 2020. <a href="https://link.springer.com/epdf/10.1007/s11042-020-08917-3">Fine grained sport action recognition with siamese spatio-temporal convolutional neural networks</a>. Multimedia Tools and Applications 79, 2020, 20429–20447.</p>

<p>[3] Pierre-Etienne Martin, Jenny Benois-Pineau, Renaud Péteri, Julien Morlier. <a href="https://hal.archives-ouvertes.fr/hal-02977646/document">3D attention mechanism for fine-grained classification of table tennis strokes using a Twin Spatio-Temporal Convolutional Neural Networks</a>. 2020 25th International Conference on Pattern Recognition (ICPR), 2021, 6019-6026.</p>

<p>[3] Gül Varol, Ivan Laptev, and Cordelia Schmid. 2018. <a href="https://arxiv.org/pdf/1604.04494.pdf">Long-Term Temporal Convolutions for Action Recognition</a>. IEEE Trans. Pattern Anal. Mach. Intell. 40, 6 (2018), 1510–1517.</p>

<p>[4] Joao Carreira and Andrew Zisserman. 2017. <a href="https://arxiv.org/pdf/1705.07750.pdf">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, 4724-4733.</p>

<p>[5] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A. Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf">AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions</a>. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, 6047-6056.</p>

<p>[6] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. <a href="https://arxiv.org/pdf/1212.0402.pdf">UCF101: A dataset of 101 human actions classes from videos in the wild</a>. Computer Vision and Pattern Recognition (cs.CV), CRCV-TR-12-01.</p>

<h4 id="task-organizers">Task organizers</h4>
<p>You can email us directly at mediaeval.sport.task (at) diff.u-bordeaux.fr</p>

<ul>
  <li>Jordan Calandre, MIA, University of La Rochelle, France</li>
  <li>Pierre-Etienne Martin, Max Planck Institute for Evolutionary Anthropology, Germany</li>
  <li>Jenny Benois-Pineau, Univ. Bordeaux, CNRS, Bordeaux INP, LaBRI, France</li>
  <li>Renaud Péteri, MIA, University of La Rochelle, France</li>
  <li>Boris Mansencal, CNRS, Bordeaux INP, LaBRI, France</li>
  <li>Julien Morlier, IMS, University of Bordeaux, France</li>
  <li>Laurent Mascarilla, MIA, University of La Rochelle, France</li>
</ul>

<h4 id="task-schedule-updated">Task Schedule (Updated)</h4>
<ul>
  <li>1 August - 15 October 2021: Data release <!-- # Replace XX with your date. We suggest setting the date in June-July--></li>
  <li>7 November 2021 <del>25 October 2021</del>: Runs due <!-- # Replace XX with your date. We suggest setting enough time in order to have enough time to assess and return the results by the Results returned deadline--></li>
  <li>8 November 2021: Results returned  <!-- Replace XX with your date. Latest possible should be 15 November--></li>
  <li>29 November 2021: Working notes paper  <!-- Fixed. Please do not change. Exact date to be decided--></li>
  <li>13-15 December 2021: MediaEval 2020 Workshop <!-- Fixed. Please do not change. Exact date to be decided--></li>
</ul>

<h4 id="acknolwedgments">Acknolwedgments</h4>
<p>We would like to thank all the players and annotators for their involvement in the acquisition and annotation processes and Alain Coupet from sports faculty of Bordeaux, expert and teacher in table tennis, for the proposed table tennis strokes taxonomy.</p>


      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
