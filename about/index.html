<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="/assets/images/editions/2019/groupphoto.jpeg" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>About MediaEval | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="About MediaEval" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<meta property="og:description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<link rel="canonical" href="https://multimediaeval.github.io//about/" />
<meta property="og:url" content="https://multimediaeval.github.io//about/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io//assets/img/mediaeval-white.png" />
<script type="application/ld+json">
{"description":"MediaEval Benchmarking Initiative Multimedia Evaluation","image":"https://multimediaeval.github.io//assets/img/mediaeval-white.png","@type":"WebSite","headline":"About MediaEval","name":"MediaEval Benchmark","url":"https://multimediaeval.github.io//about/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    


<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2020/" style="color:white;">MediaEval 2020</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
              <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
              <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2020/" style="color:white;">MediaEval 2020</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
        <article >
  <header id="main" style="background-image: url('')">

    

  </header>
  <section class="post-content">
  
	<h2 id="what-is-mediaeval">What is MediaEval?</h2>

<p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks. MediaEval attracts participants who are interested in multimodal approaches to multimedia involving, e.g., speech recognition, multimedia content analysis, music and audio analysis, user-contributed information (tags, tweets), viewer affective response, social networks, temporal and geo-coordinates.</p>

<p>For more information about past years see:</p>

<p><a href="https://records.sigmm.org/2020/07/08/mediaeval-multimedia-evaluation-benchmark-tenth-anniversary-and-counting/">SIGMM Records Volume 12, Issue 2, June 2020</a><br />
<a href="https://youtu.be/3eE8A2mC2aw">MediaEval 2017 Overview of the Year</a> <br />
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7849098">IEEE Multimedia Vol. 24, No. 1, 93-96, 2016 The Benchmarking Initiative for Multimedia Evaluation: MediaEval 2016</a> <br />
<a href="http://ercim-news.ercim.eu/en97/events/mediaeval-2013-evaluation-campaign">ERCIM News 97, April 2014</a> <br />
<a href="http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2014-02/sltc-newsletter-february-2014-mediaeval-2013/">IEEE Speech &amp; Language Processing Technical Committee Newsletter Feb. 2014</a> <br />
<a href="http://records.sigmm.ndlab.net/2013/07/mediaeval-multimedia-benchmark-highlights-from-the-ongoing-2013-season/">SIGMM Records Volume 5, Issue 2, June 2013</a> <br />
<a href="http://ercim-news.ercim.eu/en94/events/mediaeval-2012-evaluation-campaign">ERCIM News 94, July 2013</a> <br />
<a href="http://ercim-news.ercim.eu/en88/events/mediaeval-2011-evaluation-campaign">ERCIM News 88, January 2012</a> <br />
<a href="http://sigmm.org/records/records1102/featured01.html">SIGMM Records Volume 3, Issue 2, June 2011</a> <br />
<a href="http://www.sigmm.org/records/records1002/featured03">SIGMM Records Volume 2, Issue 2, June 2010</a></p>

<h2 id="what-is-the-mediaeval-workshop">What is the MediaEval Workshop?</h2>

<p>The culmination of the yearly MediaEval benchmarking cycle is the MediaEval Workshop. The workshop brings together task participants to present their findings, discuss their approaches, learn from each other, and make plans for future work.</p>

<p>The MediaEval Workshop co-located itself with ACM Multimedia conferences in 2010, 2013, 2016, 2020 and with the European Conference on Computer Vision in 2012. It was an official satellite event of Interspeech conferences in 2011 and 2015. In 2017, CLEF and MediaEval were held back to back with a overlapping joint session. Since 2017, MediaEval has been offering opportunities for remote participation and in 2020 the workshop will take place fully online.</p>

<p>Each year, the workshop publishes a working notes proceedings containing papers written by the task organizers and task participants. The aims of the MediaEval Working Notes Proceedings are described in more detail <a href="http://ceur-ws.org/Vol-1436/Paper90.pdf">in this paper</a>.</p>

<p>The MediaEval workshop also welcomes attendees who did not participate in specific tasks, but who are interested in multimedia research, or getting involved in MediaEval in the future.</p>

<h2 id="how-can-i-get-involved">How can I get involved?</h2>

<p>MediaEval is an open initiative, meaning that any interested research group is free to signup and participate. Groups sign up for one or more tasks, they then receive task definitions, data sets and supporting resources, which they use to develop their algorithms. At the very end of the summer, groups submit their results and in the fall they attend the MediaEval workshop. See also Why Participate? or watch some video on the <a href="http://www.multimediaeval.org/video/index.html">MediaEval video page</a>.</p>

<h3 id="why-participate-">Why participate ?</h3>

<p><strong>MediaEval is innovative:</strong> Expand your research horizons by trying out a new task. We make every effort to keep the threshold low for entering new tasks.</p>

<p><strong>MediaEval is flexible:</strong> Choose the tasks that interest you. Design your own combination of visual, text, speech, audio and social features for each task.</p>

<p><strong>MediaEval is both competitive and supportive:</strong> Develop or refine your techniques by comparing your results to those of others. Both newbies as well as seasoned researchers participate.</p>

<p><strong>MediaEval brings researchers together:</strong> Meet other people working on similar topics.</p>

<p><strong>MediaEval strengthens projects:</strong> Join MediaEval as a task organizer. Propose a new task based on your project.</p>

<p><strong>MediaEval is cost effective:</strong> We bundle resources to keep costs low. Participation in MediaEval is free of charge and we are making every effort to make the yearly workshop reasonably priced.</p>

<p><strong>MediaEval lets you see the world:</strong> MediaEval (previously VideoCLEF) has held events in some pretty interesting places!</p>

<h2 id="what-is-a-mediaeval-task">What is a MediaEval task?</h2>
<p>A MediaEval task consists of four parts:</p>
<ul>
  <li>Data provided to the benchmark participants,</li>
  <li>A task definition that describes the problem to be solved,</li>
  <li>Ground truth against which participants’ algorithms are evaluated,</li>
  <li>An evaluation metric.</li>
</ul>

<p>MediaEval tasks are oriented towards user needs in specific application settings and, to the extent possible, are based on scenarios of use derived from real-world problems.</p>

<h2 id="who-runs-mediaeval">Who runs MediaEval?</h2>
<p>MediaEval is a community-driven benchmark that is run by the MediaEval organizing committee consisting of the task organizers of all the individual task in a given year.</p>

<p>Martha Larson and Gareth Jones founded MediaEval in 2008 as VideoCLEF, a track in the <a href="http://www.clef-campaign.org/">CLEF Campaign</a>. Martha Larson serves as the overall contact person and the organizing force behind the MediaEval Workshop.</p>

<p>MediaEval became an independent benchmarking initiative in 2010 under the auspices of the PetaMedia Network of Excellence. In 2011, it also received support from ICT Labs of EIT. It has also received support from various sources in order to offer student travel grants. Since 2012, MediaEval has run as a fully bottom-up benchmark, in that it is not associated with a single “parent project”.</p>

<p>For support over the years we would particularly like to thank the <a href="http://elias-network.eu/">ELIAS</a> (Evaluating Information Access Systems), an ESF Research Networking Programme, the [ACM SIGIR Special Interest Group on Information Retrieval] (http://sigir.org/), and the <a href="http://sigmm.org/">ACM SIGMM Special Interest Group on Multimedia</a>. Please refer to the pages of the individual years for complete lists of supporters.</p>

<h2 id="what-is-a-mediaeval-task-force">What is a MediaEval Task Force?</h2>
<p>Proposing a task requires creating a task organization team, creating a task design (task definition that fits the user scenario, evaluation methodology) and laying the ground work for task logistics (source of data, source of ground truth, evaluation metric). MediaEval Task Forces are groups of people informally working towards a task to be proposed in a future year of MediaEval.</p>

  
  </section>

</article>

      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
