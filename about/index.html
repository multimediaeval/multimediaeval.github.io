<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>About MediaEval | MediaEval Benchmark</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="About MediaEval" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<meta property="og:description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<link rel="canonical" href="https://multimediaeval.github.io/about/" />
<meta property="og:url" content="https://multimediaeval.github.io/about/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://multimediaeval.github.io/about/","headline":"About MediaEval","description":"MediaEval Benchmarking Initiative Multimedia Evaluation","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","name":"MediaEval Benchmark","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    <div>
    


<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2022/" style="color:white;">MediaEval 2022</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                About MediaEval<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
              <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2022/" style="color:white;">MediaEval 2022</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    About MediaEval<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

</div>

    

    <div class="container">
      <div class="row">
        <article >
  <header id="main" style="background-image: url('')">

    

  </header>
  <section class="post-content">
  
	<h3 id="what-is-mediaeval">What is MediaEval?</h3>

<p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia analysis, retrieval and exploration. It emphasizes the ‘multi’ in multimedia (by involving multiple modalities such as visual, textual, audio, and sensor data) and focuses on human and social aspects of multimedia tasks. Our larger aim is to promote reproducible research that makes multimedia a positive force for society. MediaEval attracts participants who are interested in multimodal approaches to multimedia involving, e.g., speech recognition, multimedia content analysis, music and audio analysis, user-contributed information (tags, tweets), viewer affective response, environmental sensing, social networks, temporal and geo-coordinates.</p>

<p>For more information about past years see:</p>

<p><a href="https://records.sigmm.org/2020/07/08/mediaeval-multimedia-evaluation-benchmark-tenth-anniversary-and-counting/">SIGMM Records Volume 12, Issue 2, June 2020</a><br />
<a href="https://youtu.be/3eE8A2mC2aw">MediaEval 2017 Overview of the Year</a> <br />
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7849098">IEEE Multimedia Vol. 24, No. 1, 93-96, 2016 The Benchmarking Initiative for Multimedia Evaluation: MediaEval 2016</a> <br />
<a href="http://ercim-news.ercim.eu/en97/events/mediaeval-2013-evaluation-campaign">ERCIM News 97, April 2014</a> <br />
<a href="http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2014-02/sltc-newsletter-february-2014-mediaeval-2013/">IEEE Speech &amp; Language Processing Technical Committee Newsletter Feb. 2014</a> <br />
<a href="http://records.sigmm.ndlab.net/2013/07/mediaeval-multimedia-benchmark-highlights-from-the-ongoing-2013-season/">SIGMM Records Volume 5, Issue 2, June 2013</a> <br />
<a href="http://ercim-news.ercim.eu/en94/events/mediaeval-2012-evaluation-campaign">ERCIM News 94, July 2013</a> <br />
<a href="http://ercim-news.ercim.eu/en88/events/mediaeval-2011-evaluation-campaign">ERCIM News 88, January 2012</a> <br />
<a href="http://sigmm.org/records/records1102/featured01.html">SIGMM Records Volume 3, Issue 2, June 2011</a> <br />
<a href="http://www.sigmm.org/records/records1002/featured03">SIGMM Records Volume 2, Issue 2, June 2010</a></p>

<h3 id="who-runs-mediaeval">Who runs MediaEval?</h3>
<p>MediaEval is a community-driven benchmark that is run by the MediaEval organizing committee consisting of the task organizers of all the individual task in a given year. MediaEval tasks are largely autonomous, and each team of task organizers is responsible for running their own tasks.</p>

<p>Two groups work in the background to guide MediaEval and keep it running smoothly.</p>

<h4 id="the-mediaeval-logistics-committee-2021">The MediaEval Logistics Committee (2021)</h4>
<ul>
  <li>Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania</li>
  <li>Steven Hicks, SimulaMet, Norway</li>
  <li>Ngoc-Thanh Nguyen, University of Information Technology, Vietnam</li>
  <li>Ricardo Manhães Savii, Dafiti Group, Brasil (Website)</li>
</ul>

<h4 id="the-mediaeval-community-council-2021">The MediaEval Community Council (2021)</h4>
<ul>
  <li>Martha Larson, Radboud University, Netherlands (Coordinator and contact person)</li>
  <li>Minh-Son Dao, National Institute of Information and Communications Technology, Tokyo, Japan</li>
  <li>Bogdan Ionescu, University Politehnica of Bucharest, Romania</li>
  <li>Gareth J. F. Jones, Dublin City University, Ireland</li>
</ul>

<h3 id="how-can-i-get-involved">How can I get involved?</h3>
<p>MediaEval is an open initiative, meaning that any interested research group is free to signup and participate. Groups sign up for one or more tasks, they then receive task definitions, data sets and supporting resources, which they use to develop their algorithms. At the very end of the season, groups submit their results and then they attend the MediaEval workshop. See also Why Participate? or watch some videos on the <a href="http://www.multimediaeval.org/video/index.html">MediaEval video page</a>.</p>

<p>MediaEval welcomes new task proposals. At the beginning of the year, a call for new task proposals appears. Groups of researchers can propose to organize a new task, or to continue a task organized in past years. Proposing a task requires creating a task organization team, creating a task design (task definition that fits the user scenario, evaluation methodology) and laying the ground work for task logistics (source of data, source of ground truth, evaluation metric). If you have an idea for a task that is not fully developed, you can propose a MediaEval Task Forces, which is a group of people informally working towards a task to be proposed in a future year of MediaEval.</p>

<h3 id="what-is-a-mediaeval-task">What is a MediaEval task?</h3>
<p>A MediaEval task consists of four parts:</p>
<ul>
  <li>Data provided to the benchmark participants,</li>
  <li>A task definition that describes the problem to be solved,</li>
  <li>Ground truth against which participants’ algorithms are evaluated,</li>
  <li>An evaluation metric.</li>
</ul>

<p>MediaEval tasks are oriented towards user needs in specific application settings and, to the extent possible, are based on scenarios of use derived from real-world problems.</p>

<h3 id="what-is-the-mediaeval-workshop">What is the MediaEval Workshop?</h3>

<p>The culmination of the yearly MediaEval benchmarking cycle is the MediaEval Workshop. The workshop brings together task participants to present their findings, discuss their approaches, learn from each other, and make plans for future work.</p>

<p>The MediaEval Workshop co-located itself with ACM Multimedia conferences in 2010, 2013, 2016, 2019 and with the European Conference on Computer Vision in 2012. It was an official satellite event of Interspeech conferences in 2011 and 2015. In 2017, CLEF and MediaEval were held back to back with a overlapping joint session. Since 2017, MediaEval has been offering opportunities for remote participation and in 2020 the workshop took place fully online.</p>

<p>Each year, the workshop publishes a working notes proceedings containing papers written by the task organizers and task participants. The aims of the MediaEval Working Notes Proceedings are described in more detail <a href="http://ceur-ws.org/Vol-1436/Paper90.pdf">in this paper</a>.</p>

<p>The MediaEval workshop also welcomes attendees who did not participate in specific tasks, but who are interested in multimedia research, or getting involved in MediaEval in the future.</p>

<h3 id="how-did-mediaeval-come-into-being">How did MediaEval come into being?</h3>
<p>Martha Larson and Gareth Jones founded MediaEval in 2008 as VideoCLEF, a track in the <a href="http://www.clef-campaign.org/">CLEF Campaign</a>. Martha Larson serves as the overall coordinator.</p>

<p>MediaEval became an independent benchmarking initiative in 2010 under the auspices of the PetaMedia Network of Excellence. In 2011, it also received support from ICT Labs of EIT. Since 2012, MediaEval has run as a fully bottom-up benchmark, in that it is not associated with a single “parent project”.</p>

<p>For support over the years we would particularly like to thank the <a href="http://elias-network.eu/">ELIAS</a> (Evaluating Information Access Systems), an ESF Research Networking Programme, the <a href="http://sigir.org/">ACM SIGIR Special Interest Group on Information Retrieval</a>, and the <a href="http://sigmm.org/">ACM SIGMM Special Interest Group on Multimedia</a>. We are grateful for the support, specifically, because it enables us to offer travel grants to students and other researchers in need of support. The <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/">Multimedia Computing Group at TU Delft</a> has made an important contribution to MediaEval, and we would especially like to thank Saskia Peters. Please refer to the pages of the individual years for complete lists of supporters.</p>

<h3 id="why-participate-">Why participate ?</h3>

<p><strong>MediaEval is innovative:</strong> Expand your research horizons by trying out a new task. We make every effort to keep the threshold low for entering new tasks.</p>

<p><strong>MediaEval is flexible:</strong> Choose the tasks that interest you. Innovate your own combination of visual, text, speech, audio, sensor and social features.</p>

<p><strong>MediaEval is both competitive and supportive:</strong> Develop or refine your techniques by comparing your results to those of others. Both newbies as well as seasoned researchers participate.</p>

<p><strong>MediaEval brings researchers together:</strong> Meet other people working on similar topics.</p>

<p><strong>MediaEval strengthens projects:</strong> Join MediaEval as a task organizer. Propose a new task based on your project.</p>

<p><strong>MediaEval is cost effective:</strong> We bundle resources to keep costs low. Participation in MediaEval is free of charge and we are making every effort to make the yearly workshop reasonably priced.</p>

<p><strong>MediaEval lets you meet new people and see the world:</strong> MediaEval welcomes new participats and also in the past has held events in some pretty interesting places!</p>

  
  </section>

</article>

      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>
