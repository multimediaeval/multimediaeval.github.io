---
layout: page
title: MediaEval Philosophy
subtitle: 
permalink: /philosophy/
---
### MediaEval Philosophy
MediaEval is a benchmark and provides standardized task descriptions, data sets, and evaluation procedures. Benchmarks make possible systematic comparison of the performance of different approaches to problems of retrieving or analyzing multimedia content. MediaEval promotes the benefits that arise from methodical comparison: identification of state-of-the-art algorithms and promotion of research progress.  

MediaEval pursues its objective of offering a benchmark to the multimedia research community within a broader philosophy that emphasizes understanding. Specifically, we seek to transcend conventional benchmarks by striving to gain qualitative insight about our data, our evaluation metrics, our ground truth or evaluation procedure, and the fundamental nature of the challenges that our tasks represent. 

Targeting such insight helps us to avoid devoting a disproportionate amount of attention to tasks for which performance is easy or obvious to measure using conventional quantitative metrics. It also encourages the researchers who participate in the benchmark to take leaps of innovation that may not pay off in the short run, but are promising in the long run. We try to identify and appreciate the value of innovative ideas before they are well developed enough to improve metrics and to prevent promising, but still immature, approaches from being abandoned prematurely.

MediaEval expresses the importantance that it places on understanding with MediaEval Distinctive Mentions (MDMs). Each year, the organizers of the tasks single out participating team that have gone beyond solving the task to making a further contribution. Recipients of MDMs are teams that stand out because their approach to the MediaEval task was highly innovative and/or delivered important insight with potential for the future. Teams that receive MDMs are not necessarily those teams that scored at the top of the task ranking/leaderboard. 

From 2013-2019, MDMs were informal and announced at the workshop but not otherwise publicized. Starting 2020, we are publishing the MDMs online (see list below). (Note: The task rankings can be found in the workshop overview presentations of the individual tasks.) 

In 2022, MediaEval introduced the Quest for Insight, which encourages explicit discussion of how participants can explore tasks in a way that goes beyond exclusive focus on the evaluation metric. We hope that the Quest for Insight will lead to many inspiring MDMs moving forward.


#### MediaEval Distinctive Mentions (MDMs)
These are the distinctive mentions awarded at MediaEval.

##### 2021
The papers, videos, and presentations have not yet been published. We will update the details as they become available.

To *Omar Meriwani (Real Sciences)*
* For: For his creativity on taking one step-forward using the task data and exploring other approaches.
* From: The organizers of Emerging News task
* Paper: [Mediaeval 2021 Emerging News: Detection of Emerging News from Live News Stream Based on Categorization of News Annotations](https://ceur-ws.org/Vol-3181/paper66.pdf)

To *Hao Hao Tan (team Mirable)*
* For: Showing the value of additional input representations describing tonality
* From: The Organizers of Emotions and Themes in Music Task
* Paper: [Semi-Supervised Music Emotion Recognition using Noisy Student Training and Harmonic Pitch Class Profiles](https://ceur-ws.org/Vol-3181/paper17.pdf)

To *Cláudio, Rui, and David (Team NewsSeek-NOVA)*
* For: Teaching a Transformer to Recognize Faces
* From: The Organizers of the News Images Task
* Paper: [NewsSeek-NOVA at MediaEval 2021: Context-enriched Multimodal Transformers For News Images Re-matching](https://ceur-ws.org/Vol-3181/paper43.pdf)

To *SELAB-HCMUS*
* For: Amount of effort and diverse out-of box approaches is testament to the fact that they achieved best results
* From: Organizers of the task: Visual Sentiment Analysis: A Natural Disaster Use-case
* Paper: [HCMUS at MediaEval 2021: Efficient methods of Metadata Embedding and Augmentation for Visual Sentiment Analysis](https://ceur-ws.org/Vol-3181/paper33.pdf)

To *Youri Peskine, Giulio Alfarano, Ismail Harrando, Paolo Papotti and Raphaël Troncy (team D2KLab)*
* For: For the best task results achieved via effective ensembling a swarm of transformer-based models
* From: Organizers of the task: FakeNews: Corona Virus and Conspiracies Multimedia Analysis Task
* Paper: [Detecting COVID-19-Related Conspiracy Theories in Tweets](https://ceur-ws.org/Vol-3181/paper65.pdf)

To *Thomas Girault, Cheikh Brahim El Vaigh, Cyrielle Mallart and Duc Hau Nguyen (team Deltamap)*
* For: For the successful use of the prompt-based learning NLP paradigm on top of toxicity trained-model
* From: Organizers of the task: FakeNews: Corona Virus and Conspiracies Multimedia Analysis Task
* Paper: [Detecting Fake News Conspiracies with Multitask and Prompt-Based Learning](https://ceur-ws.org/Vol-3181/paper68.pdf)

To *Zeynep Pehlivan (team FakeINA)*
* For: For the fresh look at the task and successful solving pure NLP task in GNN domain
* From: Organizers of the task: FakeNews: Corona Virus and Conspiracies Multimedia Analysis Task
* Paper: [On The Pursuit of Fake News : Graph Neural Network meets NLP](https://ceur-ws.org/Vol-3181/paper61.pdf)

To *Muhammad Asif Ayub, Khubaib Ahmad, Kashif Ahmad, Nasir Ahmad, and Ala Al-Fuqaha (CSEI team)*
* For: Their idea to translate Italian tweets with positive label to English and then back to Italian to increase and balance the dataset
* From: Organizers of the task WaterMM: Water Quality in Social Multimedia
* Paper: [Deep Models for Visual Sentiment Analysis of Disaster-related Multimedia Content](https://ceur-ws.org/Vol-3181/paper22.pdf)

To *Yijun Qian, Lijun Yu, Wenhe Liu and Alexander Hauptmann*
* For: Their ablation method, quality paper and top rank in the task
* From: Organizers of the Sports Video task
* Paper: [Learning Unbiased Transformer for Long-Tail Sports Action Classification](https://ceur-ws.org/Vol-3181/paper52.pdf)

To *Alison Reboud, Ismail Harrando, Jorma Laaksonen and Raphaël Troncy*
* For: For exploring interesting concepts such as perplexity and achieving top scores in a number of subtasks.
* From: Organizers of the task: Predicting Media Memorability
* Paper: [Exploring Multimodality, Perplexity and Explainability for Memorability Prediction](https://ceur-ws.org/Vol-3181/paper53.pdf)

To *Ali Akbar, Muhammad Atif Tahir, Muhammad Rafi*
* For: Their detailed preprocessing and validation steps, taking risk to attempt all three subtasks, and top ranking in the task.
* From: Organizers of the task: Insight for Wellbeing: Cross-Data Analytics for (transboundary) Haze Prediction
* Paper: [Towards Time Series Forecasting of Cross-Data Analytics for Haze Prediction](https://ceur-ws.org/Vol-3181/paper39.pdf)

To *Andrea Storås (SimulaMet)*
* For: For her unique submission on generating segmentation masks unsupervised.
* From: Organizers of the task: Medico: Transparency in Medical Image Segmentation
* Paper: [Unsupervised Image Segmentation via Self-Supervised Learning Image Classification](https://ceur-ws.org/Vol-3181/paper12.pdf)

To *Felicia Ly Jacobsen (University of Oslo)*
* For: For making her submission more transparent by measuring uncertainty in the predicted segmentation masks.
* From: Organizers of the task: Medico: Transparency in Medical Image Segmentation
* Paper: [Predictive Uncertainty Masks from Deep Ensembles in Automated Polyp Segmentation](https://ceur-ws.org/Vol-3181/paper25.pdf)

##### 2020

To *SAIL-MiM-USC*
* For: Using different loss functions to overcome the dataset imbalance and showing importance of optimization.
* From: Emotions and Themes in Music Task organizers
* Paper: [MediaEval 2020 Emotion and Theme Recognition in Music Task: Loss Function Approaches for Multi-label Music Tagging](http://ceur-ws.org/Vol-2882/paper67.pdf)
* Presentation: [SlideShare link](https://www.slideshare.net/multimediaeval/mediaeval-2020-emotion-and-theme-recognition-in-music-task-loss-function-approaches-for-multilabel-music-tagging)
* Video: [YouTube link](https://youtu.be/5XN9345xx-I)

To *Linkmedia*
* For: Their overall performance in both text and graph classification by using artificial examples and user reputation.
* From: FakeNews Task organizers
* Paper: [Detecting Fake News in Tweets from Text and Propagation Graph: IRISA’s Participation to the FakeNews Task at MediaEval 2020](http://ceur-ws.org/Vol-2882/paper63.pdf)
* Video: [YouTube link](https://youtu.be/NjTAnRp2BMM)

To *MG-UCB*
* For: Their creative graph sampling approach exploring retweet cascade properties in small subgraphs.
* From: FakeNews Task organizers
* Paper: [Detecting Conspiracy Theories from Tweets: Textual and Structural Approaches](http://ceur-ws.org/Vol-2882/paper68.pdf)

To *DL-TXST*
* For: Their implementation of auxiliary optical character recognition for text classification and creation of website with an overview of task-related information and resources.
* From: FakeNews Task organizers
* Paper: [Enriching Content Analysis of Tweets Using Community Discovery Graph Analysis](http://ceur-ws.org/Vol-2882/paper66.pdf) 
* Video: [YouTube link](https://youtu.be/8RTPZce4O50)

To *FIG*
* For: For using only textual information and for tackling class imbalance with random undersampling instead of oversampling.
* From: Flood-related Multimedia organizers
* Paper: [A Tweet Text Binary Artificial Neural Network Classifier](http://ceur-ws.org/Vol-2882/paper25.pdf)

To *FAST-NU-DS*
* For: For using fused textual and visual information and for ensembling different classifiers.
* From: Flood-related Multimedia organizers
* Paper: [An Ensemble Based Method for the Classification of Flooding Event Using Social Media Data](http://ceur-ws.org/Vol-2882/paper37.pdf)
* Presentation: [SlideShare link](https://www.slideshare.net/multimediaeval/ensemble-based-method-for-the-classification-of-flooding-event-using-social-media-data)
* Video: [YouTube link](https://youtu.be/4ROoOzdQzEI)

To *AISIA*
* For: Their overall scores and the best utilization of lifelog data characteristics.
* From: Insight for Wellbeing Task organizers
* Paper: [A2QI: An Approach for Air Pollution Estimation in MediaEval 2020](http://ceur-ws.org/Vol-2882/paper23.pdf)
* Presentation: [SlideShare link](https://www.slideshare.net/multimediaeval/a2qi-an-approach-for-air-pollution-estimation-in-mediaeval-2020)
* Video: [YouTube link](https://youtu.be/Q3YVnQUbR0w)

To *QHL-UIT*
* For: Their simple but efficient approach for capturing spatio-temporal-concept correlation of cross-data.
* From: Insight for Wellbeing Task organizers
* Paper: [Insights for Wellbeing: Predicting Personal Air Quality Index Using Regression Approach](http://ceur-ws.org/Vol-2882/paper51.pdf)

To *AI-JMU*
* For: Not only showing where their method succeeded, but also showing where it failed.
* From: Medico Task Organizers
* Paper: [Bigger Networks are not Always Better: Deep Convolutional Neural Networks for Automated Polyp Segmentation](http://ceur-ws.org/Vol-2882/paper12.pdf)

To *All Medico Participants*
* For: Making this the biggest year for Medico despite this year's challenges and it being online.
* From: Medico Task Organizers

To *DCU-Audio*
* For: Their overall performance and the use of Memento dataset as external data.
* From: Predicting Media Memorability Task organisers
* Paper: [Leveraging Audio Gestalt to Predict Media Memorability](http://ceur-ws.org/Vol-2882/paper43.pdf)
* Video: [YouTube link](https://youtu.be/WNZ9erlzCWA)

To *MG-UCB*
* For: Their good results and their use of multiple sources (audio, text, visual) in their approach.
* From: Predicting Media Memorability Task organisers
* Paper: [Multi-Modal Ensemble Models for Predicting Video Memorability](http://ceur-ws.org/Vol-2882/paper53.pdf)
* Video: [YouTube link](https://youtu.be/qJZkNFYgE-I)

To *HCMUS_Team*
* For: Their novel approach to achieving both the adversarial effects and image enhancement in an end-to-end manner by optimizing an image-to-image translation model.
* From: Pixel Privacy Task Organizers
* Paper: [HCMUS at Pixel Privacy 2020: Quality Camouflage with Back Propagation and Image Enhancement](http://ceur-ws.org/Vol-2882/paper41.pdf)

To *Linkmedia*
* For: Their novel attempt to specifically address the JPEG compression by quantization in the DCT domain.
* From: Pixel Privacy Task Organizers
* Paper: [Fooling an Automatic Image Quality Estimator](http://ceur-ws.org/Vol-2882/paper45.pdf)

To *iCV-UT*
* For: Their proposed cascade method based on stroke decomposition for their classification.
* From: Sports Video Classification Task organizers
* Paper: [Spatio-Temporal Based Table Tennis Hit Assessment Using LSTM Algorithm](http://ceur-ws.org/Vol-2882/paper55.pdf)

To *Toyohashi University of Technology*
* For: Their ablation study on stroke classification using different sources of information.
* From: Sports Video Classification Task organizers
* Paper: [Leveraging Human Pose Estimation Model for Stroke Classification in Table Tennis](http://ceur-ws.org/Vol-2882/paper17.pdf)
