<!DOCTYPE html>

<html lang="en">
  <head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="/assets/img/mediaeval-favicon.png" type="image/png">
  <!--Import Google Icon Font-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="/assets/css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="/assets/css/main.css"  media="screen,projection"/>
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="twitter:card" content="summary" /> <!--<meta name="twitter:card" content="summary_large_image" />-->
  <meta name="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card-summary.png"/>
  <meta name="twitter:image:alt" content="Picture of group of people participants of the 2019 MediaEval workshop"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  

  <!-- seo tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MediaEval Benchmark | MediaEval Benchmarking Initiative Multimedia Evaluation</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="MediaEval Benchmark" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<meta property="og:description" content="MediaEval Benchmarking Initiative Multimedia Evaluation" />
<link rel="canonical" href="https://multimediaeval.github.io/" />
<meta property="og:url" content="https://multimediaeval.github.io/" />
<meta property="og:site_name" content="MediaEval Benchmark" />
<meta property="og:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="og:type" content="website" />
<link rel="next" href="https://multimediaeval.github.io/blog/page2" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://multimediaeval.github.io/assets/img/twitter-card.png" />
<meta property="twitter:title" content="MediaEval Benchmark" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"MediaEval Benchmarking Initiative Multimedia Evaluation","headline":"MediaEval Benchmark","image":"https://multimediaeval.github.io/assets/img/twitter-card.png","name":"MediaEval Benchmark","url":"https://multimediaeval.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Manual seo tags -->

</head>

  <body>

    




<div class="navbar-fixed">
  <nav>
    <div class="nav-wrapper green darken-4">
      <a href="/" class="brand-logo" style="padding-left: 20px">
        <h3 class="northumbria" style="margin-top: 20px;color:white;">Mediaeval</h3>
      </a>
      <!-- Small screen definition -->
      <a href="#" data-target="mobile-nav" class="sidenav-trigger">
        <i class="material-icons" style="color:white;">menu</i>
      </a>
      <!-- Big screen Structure -->
      <ul class="materialize right hide-on-med-and-down">
        <li><a href="/"><i class="material-icons" style="color:white;">home</i></a></li>
        
          <li><a href="/editions/2022/" style="color:white;">MediaEval 2022</a></li>
        
        
          
            
              <li><a class="dropdown-trigger" data-target="editions" style="color:white;">
                MediaEval History<i class="material-icons right">arrow_drop_down</i>
              </a></li>
            
          
        
          
            
                <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
            
          
        
          
            
                <li><a href="/about/" style="color:white;">About MediaEval</a></li>
            
          
        
          
            
                <li><a href="/bib/" style="color:white;">Bibliography</a></li>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </div>
  </nav>
</div>

<!-- Big screen dropdown Structure -->
<ul class="dropdown-content materialize" id="editions">
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<!-- Mobile Structure -->
<ul class="materialize sidenav green darken-4" id="mobile-nav">
  <li><a href="/" style="color:white;">Home</a></li>
  
  <li><a href="/editions/2022/" style="color:white;">MediaEval 2022</a></li>
  
  
    
      
  <li><a class="dropdown-trigger" data-target="mobile-editions" style="color:white;">
    MediaEval History<i class="material-icons right" style="color:white;">arrow_drop_down</i>
  </a></li>
      
    
  
    
      
  <li><a href="/philosophy/" style="color:white;">MediaEval Philosophy</a></li>
      
    
  
    
      
  <li><a href="/about/" style="color:white;">About MediaEval</a></li>
      
    
  
    
      
  <li><a href="/bib/" style="color:white;">Bibliography</a></li>
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</ul>

<!-- Mobile dropdown Structure -->
<ul class="dropdown-content materialize" id="mobile-editions">
  
  <li><a href="/editions/2021/">MediaEval 2021</a></li>
  
  <li><a href="/editions/2020/">MediaEval 2020</a></li>
  
  <li><a href="http://www.multimediaeval.org/" target="_blank">pre-2020</a></li>
</ul>

<header class="jumbotron">
  <div class="content">
    <h1 class='center white-text northumbria'>
      Multimedia Evaluation Benchmark
    </h1>
  </div>
</header>

    

    

    <div class="container">
      <div class="row">
        <article >

          <section class="post-content" align="left">
          
          <div class="posts">
    
    <div>
        <span>
          <header>
            <h4>
                MediaEval 2023 Call for Task Proposals
            </h4>
            <div class="post-info">
    <p class="meta">
      January 20, 2023
    </p>
</div>

          </header>
          <div>
              <p>The Multimedia Evaluation Benchmark, MediaEval, offers challenges in artificial intelligence related to data that includes multiple modalities (e.g., audio, visual, textual, and/or contextual). The goal of MediaEval is to develop and evaluate new algorithms and technologies for multimedia retrieval, access, exploration and analysis. MediaEval pursues a “Quest for Insight”: we push beyond improving evaluation scores to achieving deeper understanding about the challenges, including data and the strengths and weaknesses of particular types of approaches. Our larger aim is to promote reproducible research that makes multimedia a positive force for society. MediaEval is now calling for proposals for tasks to run in the 2023 benchmarking season.</p>

<ul>
  <li>Early bird deadline: 3 March 2023</li>
  <li>Final deadline: 14 April 2023</li>
</ul>

<p>The proposal should describe the motivation of the task, including a description of the use scenario in which the results of the tasks would be used (e.g., application that serves users). It should provide a definition of the specific problem that task participants are required to solve. Also, it should include information on the data (including source and licensing), and on how the solutions developed by task participants will be evaluated (the metric and description of how the metric is related to the use scenario). We ask you to think carefully about specific research questions that are related to the challenge, and mention these in the proposal. These research questions will guide participants in pursuing the “Quest for Insight”, i.e., going beyond thinking only about evaluation scores. Finally, the proposal must also include a statement of how the task is related to MediaEval (i.e., the human or social component), and how it extends the state of the art.</p>

<h5 id="indication-of-intent">Indication of Intent</h5>

<p>If you plan to submit a task proposal, we strongly suggest that you submit, by email, an “Indication of Intent” in the form of a short task summary (a blurb of 50-100 words) as soon as possible. The description should include a clear statement of what participants are expected to do, which data is used, and how participant submissions are evaluated. The summary should finish with a statement of the motivation for the task.</p>

<h5 id="full-task-proposal">Full Task Proposal</h5>

<p>A task proposal contains the following elements. Note that there is no specified length for the proposal, but in general proposals do not exceed three pages.</p>

<h6 id="part-i-task-description">Part I: Task Description</h6>

<p>This is a version of your task description that will be posted to the MediaEval website. Its goal is to inform and attract the interest of potential participants. It consists of the following parts:</p>

<ul>
  <li>Task title: Give your task an informative title.</li>
  <li>Task description: State the goal of the task and what is expected of task participants in a simple easy-to-understand manner. The task description should make clear what the task requires of participants.</li>
  <li>Motivation and background: Describe the motivating use scenario, i.e., how would the results of the task be used in an application that serves users. Also, state how the task extends the state of the art.</li>
  <li>Target group: Describe the type of researchers who would be interested in participating in the task.</li>
  <li>Data: Describe the data, including how the data will be collected and licensed.</li>
  <li>Evaluation methodology: Describe the evaluation methodology, including how the ground truth will be created and the evaluation metrics.</li>
  <li>“Quest for Insight”: List several research questions related to the challenge, which the participants can strive to answer in order to go beyond just looking at evaluation metrics.</li>
  <li>References and recommended reading: list 3-4 references related to the task that teams should have read before attempting the task.</li>
  <li>List of task organizers. (Designate a lead task organizer whose contact details will appear on the website for the task.) For example task descriptions, please see the task pages of the MediaEval 2021 Tasks.</li>
</ul>

<h6 id="part-ii-big-picture-of-the-task">Part II: Big Picture of the Task</h6>

<p>Please address each of the following points with 2-3 sentences each:</p>

<p>Innovation: MediaEval strives to offer innovative tasks. New tasks open up new terrain for multimedia researchers, continuing tasks introduce novel aspects every year that drive forward the state of the art. Describe the novel contribution of your task.
Focus: MediaEval focuses on tasks that have a human or social aspect. This means that they serve groups of users, work with multimedia content produced by users, and/or address issues of affect and subjectivity. MediaEval strives to promote reproducible research that makes multimedia a positive force for society. Please comment on the human or social aspect of your task.
Risk management: What are the main risks that you foresee for the task, and how you plan to address them (i.e., what challenges will you face in organizing the task and how do you expect to overcome them)?</p>

<h6 id="part-iii-task-organization-team">Part III: Task Organization Team</h6>

<p>Write a very brief paragraph outlining the relevant interests and experience of your organizing team. Your team should be large enough to handle the organization and management of the task. This includes evaluating participant runs, and carrying out failure analysis on the results. Ideally teams should consist of members from multiple research sites and multiple projects. A mix of experienced and early-career researchers is preferred. MediaEval has a strong tradition of encouraging and supporting early-stage researchers in gaining experience in organization of benchmark tasks. Note that your task team can add members after the proposal has been accepted.</p>

<h5 id="submission">Submission</h5>

<p>Please submit your proposal (as a text file, .doc, .docx or link to an editable Google doc file) by emailing it to Martha Larson m (dot) larson (at) cs (dot) ru (dot) nl with Steven Hicks steven (at) simula (dot) no and Mihai Gabriel (Gabi) Constantin mihai.constantin84 (at) upb (dot) ro on cc.</p>

<h5 id="mediaeval-2023-schedule">MediaEval 2023 Schedule</h5>

<p>May-June: Validation data release
June-August: Test data related
End October: Runs due
End November: Working notes papers due
January 2024: MediaEval 2023 Workshop, Collocated with MMM 2024 in Amsterdam, Netherlands and also online.</p>

          </div>
      </span>
    </div>
    <hr />
    
    
    <div>
        <span>
          <header>
            <h4>
                MediaEval Workshop Information
            </h4>
            <div class="post-info">
    <p class="meta">
      January 6, 2023
    </p>
</div>

          </header>
          <div>
              <p>The MediaEval 2022 workshop will take place Thursday-Friday 12-13 January...
              <a href="/2023/01/06/workshop-information.html">read more</a>
          </div>
      </span>
    </div>
    <hr />
    
    <div>
        <span>
          <header>
            <h4>
                MediaEval 2022 Workshop Registration
            </h4>
            <div class="post-info">
    <p class="meta">
      November 16, 2022
    </p>
</div>

          </header>
          <div>
              <p>MediaEval 2022 workshop Bergen, Norway, 12-13 January 2023, is co-located...
              <a href="/2022/11/16/workshop-registration.html">read more</a>
          </div>
      </span>
    </div>
    <hr />
    
    <div>
        <span>
          <header>
            <h4>
                MediaEval 2022 Registration
            </h4>
            <div class="post-info">
    <p class="meta">
      June 16, 2022
    </p>
</div>

          </header>
          <div>
              <p>The Benchmarking Initiative for Multimedia Evaluation (MediaEval) offers challenges related...
              <a href="/2022/06/16/registration-2022.html">read more</a>
          </div>
      </span>
    </div>
    <hr />
    
    <div>
        <span>
          <header>
            <h4>
                MediaEval 2022 Call for Task Proposals
            </h4>
            <div class="post-info">
    <p class="meta">
      January 26, 2022
    </p>
</div>

          </header>
          <div>
              <p>The Multimedia Evaluation Benchmark, MediaEval, offers challenges in artificial intelligence...
              <a href="/2022/01/26/call.html">read more</a>
          </div>
      </span>
    </div>
    <hr />
    
</div>





<ul class="pagination">
    
        <li class="disabled"><a><i class="material-icons">chevron_left</i></a></li>
    

    
        
            
                <li class="active"><a href="/">1</a></li>
            
        
    
        
            <li class="waves-effect"><a href="/blog/page2">2</a></li>
        
    
        
            <li class="waves-effect"><a href="/blog/page3">3</a></li>
        
    

    
        <li class="waves-effect"><a href="/blog/page2"><i class="material-icons">chevron_right</i></a></li>
    
</ul>



          
          </section>

        </article>
      </div>
    </div>

    
<footer class='page-footer green darken-4'>
  <div class='container'>

    <div class='row'>
      <div class="col l6 s12">
        <h5 class="white-text">What is MediaEval?</h5>
        <p>MediaEval is a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval. 
          It emphasizes the ‘multi’ in multimedia and focuses on human and social aspects of multimedia tasks.</p>

        <p>For more information contact Martha Larson m.larson (at) cs.ru.nl</p>
      </div>
      <div class="col l4 offset-l2 s12">
        <h5 class="white-text">Links</h5>
        <a href='https://twitter.com/multimediaeval' target="_blank">
          <i class="fa fa-twitter fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://github.com/multimediaeval' target="_blank">
          <i class="fa fa-github fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.flickr.com/photos/69524595@N06/' target="_blank">
          <i class="fa fa-flickr fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
        <a href='https://www.youtube.com/channel/UCc-1NW1Uo2o_zI4F81iyTcw' target="_blank">
          <i class="fa fa-youtube fa-3x" aria-hidden="true" style="color:white;"></i>
        </a>
      </div>
    </div>

  </div>
    <div class="footer-copyright">
      <div class="container grey-text text-lighten-4">
      &copy; 2020 MediaEval Multimedia Benchmark
    </div>
  </div>
</footer>


    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="/assets/js/materialize.min.js"></script>
    <script type="text/javascript" src="/assets/js/main.js"></script>
  </body>

</html>

